{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# For neural networks.\n",
    "import keras as kr\n",
    "# For data frames.\n",
    "import pandas as pd\n",
    "# For numerical arrays.\n",
    "import numpy as np\n",
    "# For preprocessing data.\n",
    "import sklearn.preprocessing as pre\n",
    "# For splitting data sets.\n",
    "import sklearn.model_selection as mod\n",
    "# For whitening.\n",
    "import sklearn.decomposition as dec\n",
    "from sklearn.datasets import load_boston #load the boston house proce index from the sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_dataset = load_boston()\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston['MEDV'] = boston_dataset.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = boston.iloc[:,0:13]\n",
    "y = boston.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = mod.train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = kr.models.Sequential()\n",
    "\n",
    "m.add(kr.layers.Dense(20, input_dim=13, activation=\"relu\"))\n",
    "m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "#m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "m.compile(loss=\"mean_squared_logarithmic_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "354/354 [==============================] - 0s 508us/step - loss: 2.0411\n",
      "Epoch 2/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 1.6628\n",
      "Epoch 3/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 1.3536\n",
      "Epoch 4/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 1.2349\n",
      "Epoch 5/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 1.0630\n",
      "Epoch 6/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.9403\n",
      "Epoch 7/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.8175\n",
      "Epoch 8/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.7099\n",
      "Epoch 9/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.6460\n",
      "Epoch 10/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.6215\n",
      "Epoch 11/100\n",
      "354/354 [==============================] - ETA: 0s - loss: 0.600 - 0s 169us/step - loss: 0.5975\n",
      "Epoch 12/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.5846\n",
      "Epoch 13/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.5765\n",
      "Epoch 14/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.5702\n",
      "Epoch 15/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.5660\n",
      "Epoch 16/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.5651\n",
      "Epoch 17/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.5602\n",
      "Epoch 18/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.5594\n",
      "Epoch 19/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.5561\n",
      "Epoch 20/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.5568\n",
      "Epoch 21/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.4943\n",
      "Epoch 22/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.3281\n",
      "Epoch 23/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.2038\n",
      "Epoch 24/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1533\n",
      "Epoch 25/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1309\n",
      "Epoch 26/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1164\n",
      "Epoch 27/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.1083\n",
      "Epoch 28/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0943\n",
      "Epoch 29/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0910\n",
      "Epoch 30/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0895\n",
      "Epoch 31/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0825\n",
      "Epoch 32/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0801\n",
      "Epoch 33/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0789\n",
      "Epoch 34/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0777\n",
      "Epoch 35/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0767\n",
      "Epoch 36/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0737\n",
      "Epoch 37/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0751\n",
      "Epoch 38/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0722\n",
      "Epoch 39/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0720\n",
      "Epoch 40/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0729\n",
      "Epoch 41/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0690\n",
      "Epoch 42/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0696\n",
      "Epoch 43/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0673\n",
      "Epoch 44/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0685\n",
      "Epoch 45/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0656\n",
      "Epoch 46/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0649\n",
      "Epoch 47/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0652\n",
      "Epoch 48/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0643\n",
      "Epoch 49/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0634\n",
      "Epoch 50/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0626\n",
      "Epoch 51/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0625\n",
      "Epoch 52/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0624\n",
      "Epoch 53/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0617\n",
      "Epoch 54/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0611\n",
      "Epoch 55/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0594\n",
      "Epoch 56/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0608\n",
      "Epoch 57/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0596\n",
      "Epoch 58/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0588\n",
      "Epoch 59/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0600\n",
      "Epoch 60/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0601\n",
      "Epoch 61/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0569\n",
      "Epoch 62/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0588\n",
      "Epoch 63/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0598\n",
      "Epoch 64/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0583\n",
      "Epoch 65/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0571\n",
      "Epoch 66/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0578\n",
      "Epoch 67/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0558\n",
      "Epoch 68/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0577\n",
      "Epoch 69/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0565\n",
      "Epoch 70/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0567\n",
      "Epoch 71/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0577\n",
      "Epoch 72/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0550\n",
      "Epoch 73/100\n",
      "354/354 [==============================] - 0s 339us/step - loss: 0.0548\n",
      "Epoch 74/100\n",
      "354/354 [==============================] - 0s 480us/step - loss: 0.0548\n",
      "Epoch 75/100\n",
      "354/354 [==============================] - 0s 689us/step - loss: 0.0561\n",
      "Epoch 76/100\n",
      "354/354 [==============================] - 0s 339us/step - loss: 0.0557\n",
      "Epoch 77/100\n",
      "354/354 [==============================] - 0s 401us/step - loss: 0.0558\n",
      "Epoch 78/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0569\n",
      "Epoch 79/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0630\n",
      "Epoch 80/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0557\n",
      "Epoch 81/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0543\n",
      "Epoch 82/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0538\n",
      "Epoch 83/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0558\n",
      "Epoch 84/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0542\n",
      "Epoch 85/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0583\n",
      "Epoch 86/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0608\n",
      "Epoch 87/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0559\n",
      "Epoch 88/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0539\n",
      "Epoch 89/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0560\n",
      "Epoch 90/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0520\n",
      "Epoch 91/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0524\n",
      "Epoch 92/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0542\n",
      "Epoch 93/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0546\n",
      "Epoch 94/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0512\n",
      "Epoch 95/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0504\n",
      "Epoch 96/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0518\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 0s 226us/step - loss: 0.0534\n",
      "Epoch 98/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0506\n",
      "Epoch 99/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0500\n",
      "Epoch 100/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0xfff9080>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(x_train, y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = pre.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = kr.models.Sequential()\n",
    "\n",
    "m.add(kr.layers.Dense(20, input_dim=13, activation=\"relu\"))\n",
    "m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "#m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "m.compile(loss=\"mean_squared_logarithmic_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "354/354 [==============================] - 0s 480us/step - loss: 8.5117\n",
      "Epoch 2/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.7500\n",
      "Epoch 3/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.4583\n",
      "Epoch 4/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.2484\n",
      "Epoch 5/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.2260\n",
      "Epoch 6/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.2142\n",
      "Epoch 7/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.2066\n",
      "Epoch 8/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.2017\n",
      "Epoch 9/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.1990\n",
      "Epoch 10/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.1890\n",
      "Epoch 11/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.1821\n",
      "Epoch 12/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1792\n",
      "Epoch 13/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1709\n",
      "Epoch 14/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1654\n",
      "Epoch 15/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1638\n",
      "Epoch 16/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1601\n",
      "Epoch 17/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.1505\n",
      "Epoch 18/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1473\n",
      "Epoch 19/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1434\n",
      "Epoch 20/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.2589\n",
      "Epoch 21/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1286\n",
      "Epoch 22/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1118\n",
      "Epoch 23/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1080\n",
      "Epoch 24/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0986\n",
      "Epoch 25/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0980\n",
      "Epoch 26/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0939\n",
      "Epoch 27/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0913\n",
      "Epoch 28/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0947\n",
      "Epoch 29/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0844\n",
      "Epoch 30/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0839\n",
      "Epoch 31/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0827\n",
      "Epoch 32/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0782\n",
      "Epoch 33/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0765\n",
      "Epoch 34/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0739\n",
      "Epoch 35/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0725\n",
      "Epoch 36/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0756\n",
      "Epoch 37/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0699\n",
      "Epoch 38/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0696\n",
      "Epoch 39/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0669\n",
      "Epoch 40/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0689\n",
      "Epoch 41/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0652\n",
      "Epoch 42/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0636\n",
      "Epoch 43/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0634\n",
      "Epoch 44/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0626\n",
      "Epoch 45/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0614\n",
      "Epoch 46/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0590\n",
      "Epoch 47/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0625\n",
      "Epoch 48/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0651\n",
      "Epoch 49/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0578\n",
      "Epoch 50/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0578\n",
      "Epoch 51/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0569\n",
      "Epoch 52/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0573\n",
      "Epoch 53/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0589\n",
      "Epoch 54/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0561\n",
      "Epoch 55/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0580\n",
      "Epoch 56/100\n",
      "354/354 [==============================] - 0s 429us/step - loss: 0.0575\n",
      "Epoch 57/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0588\n",
      "Epoch 58/100\n",
      "354/354 [==============================] - 0s 373us/step - loss: 0.0581\n",
      "Epoch 59/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0559\n",
      "Epoch 60/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0554\n",
      "Epoch 61/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0564\n",
      "Epoch 62/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0570\n",
      "Epoch 63/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0529\n",
      "Epoch 64/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0566\n",
      "Epoch 65/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0567\n",
      "Epoch 66/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0526\n",
      "Epoch 67/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0535\n",
      "Epoch 68/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0567\n",
      "Epoch 69/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0548\n",
      "Epoch 70/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0556\n",
      "Epoch 71/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0540\n",
      "Epoch 72/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0527\n",
      "Epoch 73/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0522\n",
      "Epoch 74/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0505\n",
      "Epoch 75/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0567\n",
      "Epoch 76/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0503\n",
      "Epoch 77/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0515\n",
      "Epoch 78/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0532\n",
      "Epoch 79/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0504\n",
      "Epoch 80/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0499\n",
      "Epoch 81/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0509\n",
      "Epoch 82/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0524\n",
      "Epoch 83/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0561\n",
      "Epoch 84/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0517\n",
      "Epoch 85/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0501\n",
      "Epoch 86/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0510\n",
      "Epoch 87/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0526\n",
      "Epoch 88/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0555\n",
      "Epoch 89/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0500\n",
      "Epoch 90/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0538\n",
      "Epoch 91/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0581\n",
      "Epoch 92/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0496\n",
      "Epoch 93/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0503\n",
      "Epoch 94/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0593\n",
      "Epoch 95/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0536\n",
      "Epoch 96/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0580\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 0s 226us/step - loss: 0.0483\n",
      "Epoch 98/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0491\n",
      "Epoch 99/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0495\n",
      "Epoch 100/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x10ee5908>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(x_train, y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.,  9., 15., 12., 18., 13., 24., 17., 12., 16., 32., 21., 25.,\n",
       "        27., 21., 15., 23., 16.,  5., 14., 11., 25., 25., 25., 27., 22.,\n",
       "        13., 22., 16., 20., 22., 17., 20.,  7., 25., 16., 22., 24., 23.,\n",
       "        21., 19., 21., 21., 25., 22., 23., 20., 27., 24., 25., 23., 17.,\n",
       "        18., 24., 16., 21., 18., 26., 17., 20., 33., 16., 16., 25., 16.,\n",
       "        14., 19., 18., 27., 20., 24., 22., 22., 13., 26., 23., 18., 27.,\n",
       "        25., 28., 19., 19., 10., 30., 36., 26., 22., 13., 15., 22., 24.,\n",
       "        24., 19., 25., 24., 23., 17., 28., 20., 23., 25., 17., 19., 23.,\n",
       "        10., 21., 26., 18., 27., 25., 12., 24., 25., 18., 17., 22., 15.,\n",
       "        19., 22., 21., 24., 21., 20., 13., 19., 27., 24., 18., 26., 16.,\n",
       "        28., 16., 14., 14., 20., 24., 23.,  7., 20., 41., 22., 29., 12.,\n",
       "        23., 21., 20., 18., 18., 18., 17., 22., 32.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(x_test).round().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27.5, 17.8, 12.7, 12. , 15.6,  8.3, 23.9, 16.1, 11.8, 20.4, 31.6,\n",
       "       22.6, 23.7, 42.3, 24. , 13.8, 29. ,  8.5, 13.2, 15.6, 14.6, 48.8,\n",
       "       50. , 24.8, 23.4, 22. , 12.8, 28.7, 17. , 22.7, 20. , 19.9, 21.2,\n",
       "        6.3, 29. , 13.4, 19.9, 25.1, 22.2, 20.3, 22. , 20.5, 30.3, 25. ,\n",
       "       25. , 21.6, 18.2, 33.4, 32. , 23.1, 24.1, 15. , 17.7, 19. ,  9.6,\n",
       "       22.9, 16.6, 37.6, 20. , 20.8, 37.3, 12.7, 14.9, 34.7, 13.6,  9.5,\n",
       "       27.5, 15.2, 35.2, 18.2, 23.9, 20.9, 23.8,  7.5, 44.8, 22.6, 16.8,\n",
       "       32. , 48.3, 50. , 19.6, 21.8,  8.4, 33.1, 30.1, 50. , 23.3, 14.8,\n",
       "       14.1, 24.5, 41.3, 24.6, 15.3, 23. , 36.5, 27.1, 17.8, 26.6, 23.1,\n",
       "       18.9, 23.7, 19.1, 17.5, 50. ,  5.6, 28.4, 36.2, 19.5, 43.5, 25. ,\n",
       "        7. , 18.7, 50. , 22.5, 19.9, 26.5, 14.1, 20. , 24.3, 20.1, 36. ,\n",
       "       21.7, 21.8, 20.2, 17.6, 50. , 29.9, 18.9, 33.3, 16.7, 50. , 12.5,\n",
       "       19.4, 13.4, 19.8, 26.6, 23.9,  8.8, 18.9, 50. , 21.7, 36.1, 17.2,\n",
       "       18.2, 20.3, 21.7, 21.4, 20.6, 13.8, 13.4, 23.1, 44. ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.as_matrix().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.779282</td>\n",
       "      <td>0.046682</td>\n",
       "      <td>-1.372281</td>\n",
       "      <td>0.526056</td>\n",
       "      <td>-0.098318</td>\n",
       "      <td>-0.518909</td>\n",
       "      <td>0.701723</td>\n",
       "      <td>-1.025190</td>\n",
       "      <td>-0.817553</td>\n",
       "      <td>1.193480</td>\n",
       "      <td>0.289122</td>\n",
       "      <td>3.665580</td>\n",
       "      <td>-0.479830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.143789</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.524269</td>\n",
       "      <td>-0.700368</td>\n",
       "      <td>0.429386</td>\n",
       "      <td>-0.554615</td>\n",
       "      <td>-0.340130</td>\n",
       "      <td>-0.950101</td>\n",
       "      <td>-1.788952</td>\n",
       "      <td>-0.816782</td>\n",
       "      <td>0.122835</td>\n",
       "      <td>-0.479731</td>\n",
       "      <td>-0.729605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.207060</td>\n",
       "      <td>-0.351878</td>\n",
       "      <td>-0.659215</td>\n",
       "      <td>-0.086648</td>\n",
       "      <td>-0.453202</td>\n",
       "      <td>-0.790520</td>\n",
       "      <td>-0.056720</td>\n",
       "      <td>1.268348</td>\n",
       "      <td>1.413974</td>\n",
       "      <td>-1.203726</td>\n",
       "      <td>-0.758255</td>\n",
       "      <td>-0.223999</td>\n",
       "      <td>-0.164471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.759802</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.801030</td>\n",
       "      <td>-2.440905</td>\n",
       "      <td>0.227237</td>\n",
       "      <td>-0.040985</td>\n",
       "      <td>0.409850</td>\n",
       "      <td>-0.319775</td>\n",
       "      <td>0.528061</td>\n",
       "      <td>-0.206056</td>\n",
       "      <td>-0.146037</td>\n",
       "      <td>-0.112363</td>\n",
       "      <td>1.137874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.192579</td>\n",
       "      <td>0.779302</td>\n",
       "      <td>-1.818313</td>\n",
       "      <td>0.794719</td>\n",
       "      <td>0.429588</td>\n",
       "      <td>3.154097</td>\n",
       "      <td>1.839061</td>\n",
       "      <td>-3.204694</td>\n",
       "      <td>0.195145</td>\n",
       "      <td>0.103449</td>\n",
       "      <td>-0.628099</td>\n",
       "      <td>-0.540744</td>\n",
       "      <td>-0.996219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.228604</td>\n",
       "      <td>0.426378</td>\n",
       "      <td>-0.979864</td>\n",
       "      <td>-0.247193</td>\n",
       "      <td>0.459192</td>\n",
       "      <td>-0.066192</td>\n",
       "      <td>-1.448980</td>\n",
       "      <td>-0.016962</td>\n",
       "      <td>-0.253414</td>\n",
       "      <td>-0.957044</td>\n",
       "      <td>0.360711</td>\n",
       "      <td>-0.347894</td>\n",
       "      <td>-0.141433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.684186</td>\n",
       "      <td>-0.017125</td>\n",
       "      <td>-0.633368</td>\n",
       "      <td>-0.390375</td>\n",
       "      <td>-0.004051</td>\n",
       "      <td>-0.724386</td>\n",
       "      <td>-0.940293</td>\n",
       "      <td>0.685945</td>\n",
       "      <td>-1.072874</td>\n",
       "      <td>-1.024015</td>\n",
       "      <td>1.553152</td>\n",
       "      <td>-0.324153</td>\n",
       "      <td>-0.547408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.832640</td>\n",
       "      <td>0.042129</td>\n",
       "      <td>-0.555234</td>\n",
       "      <td>-0.578430</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>-1.195204</td>\n",
       "      <td>-0.396563</td>\n",
       "      <td>1.114157</td>\n",
       "      <td>-0.122953</td>\n",
       "      <td>-0.906826</td>\n",
       "      <td>-0.867879</td>\n",
       "      <td>-0.323447</td>\n",
       "      <td>-0.036915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.525737</td>\n",
       "      <td>-0.050751</td>\n",
       "      <td>1.643198</td>\n",
       "      <td>-1.720299</td>\n",
       "      <td>0.040105</td>\n",
       "      <td>-0.153797</td>\n",
       "      <td>0.232904</td>\n",
       "      <td>-0.324561</td>\n",
       "      <td>0.808827</td>\n",
       "      <td>2.192158</td>\n",
       "      <td>-1.191256</td>\n",
       "      <td>-0.089669</td>\n",
       "      <td>0.977007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.135853</td>\n",
       "      <td>0.079028</td>\n",
       "      <td>-0.702650</td>\n",
       "      <td>0.183677</td>\n",
       "      <td>-0.684583</td>\n",
       "      <td>0.049694</td>\n",
       "      <td>-0.398385</td>\n",
       "      <td>1.708434</td>\n",
       "      <td>-0.563914</td>\n",
       "      <td>-0.600616</td>\n",
       "      <td>0.373670</td>\n",
       "      <td>-0.109243</td>\n",
       "      <td>-0.844511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.823014</td>\n",
       "      <td>0.072077</td>\n",
       "      <td>-0.853458</td>\n",
       "      <td>-0.163122</td>\n",
       "      <td>-0.010287</td>\n",
       "      <td>-1.006865</td>\n",
       "      <td>-0.681461</td>\n",
       "      <td>1.204574</td>\n",
       "      <td>-0.113234</td>\n",
       "      <td>-0.935417</td>\n",
       "      <td>0.390315</td>\n",
       "      <td>-0.286094</td>\n",
       "      <td>-0.394194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.094559</td>\n",
       "      <td>0.201951</td>\n",
       "      <td>-0.257788</td>\n",
       "      <td>1.426358</td>\n",
       "      <td>0.237420</td>\n",
       "      <td>-1.187836</td>\n",
       "      <td>-1.123423</td>\n",
       "      <td>-1.294700</td>\n",
       "      <td>0.594517</td>\n",
       "      <td>-1.796983</td>\n",
       "      <td>-0.944997</td>\n",
       "      <td>-0.699057</td>\n",
       "      <td>-0.328123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.532247</td>\n",
       "      <td>-0.143635</td>\n",
       "      <td>1.335205</td>\n",
       "      <td>-1.305557</td>\n",
       "      <td>0.080791</td>\n",
       "      <td>0.128988</td>\n",
       "      <td>-0.074948</td>\n",
       "      <td>-0.255016</td>\n",
       "      <td>0.676137</td>\n",
       "      <td>1.453431</td>\n",
       "      <td>-0.877885</td>\n",
       "      <td>-0.094830</td>\n",
       "      <td>0.484352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.032189</td>\n",
       "      <td>-0.364214</td>\n",
       "      <td>0.984574</td>\n",
       "      <td>-2.175333</td>\n",
       "      <td>-0.364840</td>\n",
       "      <td>0.176266</td>\n",
       "      <td>-0.595259</td>\n",
       "      <td>2.924795</td>\n",
       "      <td>-1.264507</td>\n",
       "      <td>-0.211471</td>\n",
       "      <td>0.523410</td>\n",
       "      <td>0.323176</td>\n",
       "      <td>-0.212280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.970383</td>\n",
       "      <td>0.111723</td>\n",
       "      <td>2.256356</td>\n",
       "      <td>1.321040</td>\n",
       "      <td>0.183366</td>\n",
       "      <td>-0.190510</td>\n",
       "      <td>0.615912</td>\n",
       "      <td>-0.924141</td>\n",
       "      <td>0.926155</td>\n",
       "      <td>-2.267388</td>\n",
       "      <td>-0.052985</td>\n",
       "      <td>-0.161320</td>\n",
       "      <td>0.619036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.802024</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>-0.519852</td>\n",
       "      <td>-0.606740</td>\n",
       "      <td>-0.001044</td>\n",
       "      <td>-1.135421</td>\n",
       "      <td>-0.529680</td>\n",
       "      <td>1.286244</td>\n",
       "      <td>-0.042890</td>\n",
       "      <td>-0.967516</td>\n",
       "      <td>-1.702434</td>\n",
       "      <td>-0.308814</td>\n",
       "      <td>-0.743266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.483395</td>\n",
       "      <td>-0.117858</td>\n",
       "      <td>-0.686213</td>\n",
       "      <td>-0.485919</td>\n",
       "      <td>0.258766</td>\n",
       "      <td>-0.056492</td>\n",
       "      <td>1.519008</td>\n",
       "      <td>-0.777113</td>\n",
       "      <td>-0.885896</td>\n",
       "      <td>0.855188</td>\n",
       "      <td>-0.045535</td>\n",
       "      <td>-0.381771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.056649</td>\n",
       "      <td>-0.550090</td>\n",
       "      <td>-0.949781</td>\n",
       "      <td>0.466610</td>\n",
       "      <td>-0.554758</td>\n",
       "      <td>0.164951</td>\n",
       "      <td>-0.562458</td>\n",
       "      <td>1.721568</td>\n",
       "      <td>-0.590728</td>\n",
       "      <td>-0.352252</td>\n",
       "      <td>-0.103668</td>\n",
       "      <td>-0.195062</td>\n",
       "      <td>-0.935088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.088007</td>\n",
       "      <td>0.197458</td>\n",
       "      <td>-0.273877</td>\n",
       "      <td>-1.154001</td>\n",
       "      <td>0.424371</td>\n",
       "      <td>-0.010448</td>\n",
       "      <td>-0.725374</td>\n",
       "      <td>-0.695716</td>\n",
       "      <td>1.137542</td>\n",
       "      <td>0.121476</td>\n",
       "      <td>0.845598</td>\n",
       "      <td>-0.256113</td>\n",
       "      <td>1.300919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.327613</td>\n",
       "      <td>-1.388404</td>\n",
       "      <td>-0.388812</td>\n",
       "      <td>0.388301</td>\n",
       "      <td>0.430668</td>\n",
       "      <td>2.602374</td>\n",
       "      <td>-1.845792</td>\n",
       "      <td>-0.406840</td>\n",
       "      <td>-0.737346</td>\n",
       "      <td>-0.781541</td>\n",
       "      <td>0.336719</td>\n",
       "      <td>-0.168536</td>\n",
       "      <td>-0.715806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.176383</td>\n",
       "      <td>-0.402101</td>\n",
       "      <td>-0.406060</td>\n",
       "      <td>-0.431329</td>\n",
       "      <td>-0.300219</td>\n",
       "      <td>0.251357</td>\n",
       "      <td>-0.253567</td>\n",
       "      <td>0.971402</td>\n",
       "      <td>0.138848</td>\n",
       "      <td>-0.905731</td>\n",
       "      <td>0.656374</td>\n",
       "      <td>-0.123651</td>\n",
       "      <td>0.668999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.307242</td>\n",
       "      <td>-1.388448</td>\n",
       "      <td>0.623496</td>\n",
       "      <td>-0.991475</td>\n",
       "      <td>-0.245434</td>\n",
       "      <td>-0.244169</td>\n",
       "      <td>0.040102</td>\n",
       "      <td>-0.944423</td>\n",
       "      <td>-0.117694</td>\n",
       "      <td>0.232065</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>-0.454907</td>\n",
       "      <td>0.281327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.887831</td>\n",
       "      <td>2.890825</td>\n",
       "      <td>0.120417</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>-0.748978</td>\n",
       "      <td>0.487974</td>\n",
       "      <td>-0.931154</td>\n",
       "      <td>-0.261225</td>\n",
       "      <td>0.161104</td>\n",
       "      <td>-0.117257</td>\n",
       "      <td>-1.707151</td>\n",
       "      <td>-0.080691</td>\n",
       "      <td>1.174269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.324486</td>\n",
       "      <td>-1.389708</td>\n",
       "      <td>-0.293408</td>\n",
       "      <td>0.284724</td>\n",
       "      <td>-0.142051</td>\n",
       "      <td>-0.198274</td>\n",
       "      <td>-0.538138</td>\n",
       "      <td>-0.667592</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.406817</td>\n",
       "      <td>-0.222981</td>\n",
       "      <td>-0.555188</td>\n",
       "      <td>0.629739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.354292</td>\n",
       "      <td>-1.157860</td>\n",
       "      <td>-0.238734</td>\n",
       "      <td>0.230908</td>\n",
       "      <td>-0.238265</td>\n",
       "      <td>-0.273054</td>\n",
       "      <td>-0.496439</td>\n",
       "      <td>-0.682741</td>\n",
       "      <td>0.014758</td>\n",
       "      <td>0.246315</td>\n",
       "      <td>-0.246031</td>\n",
       "      <td>3.826799</td>\n",
       "      <td>2.059421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.794372</td>\n",
       "      <td>0.043294</td>\n",
       "      <td>0.341218</td>\n",
       "      <td>-1.828265</td>\n",
       "      <td>0.292837</td>\n",
       "      <td>0.393407</td>\n",
       "      <td>-0.381963</td>\n",
       "      <td>-0.091684</td>\n",
       "      <td>0.104467</td>\n",
       "      <td>-1.342048</td>\n",
       "      <td>0.543630</td>\n",
       "      <td>-0.093569</td>\n",
       "      <td>0.582312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.963710</td>\n",
       "      <td>0.126650</td>\n",
       "      <td>-0.173215</td>\n",
       "      <td>-1.211778</td>\n",
       "      <td>0.366479</td>\n",
       "      <td>0.521503</td>\n",
       "      <td>-0.821885</td>\n",
       "      <td>0.342614</td>\n",
       "      <td>-0.061456</td>\n",
       "      <td>-0.530902</td>\n",
       "      <td>0.169190</td>\n",
       "      <td>-0.101348</td>\n",
       "      <td>-0.349721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.335881</td>\n",
       "      <td>-1.378706</td>\n",
       "      <td>-0.142699</td>\n",
       "      <td>0.172779</td>\n",
       "      <td>9.548751</td>\n",
       "      <td>-2.106839</td>\n",
       "      <td>3.086271</td>\n",
       "      <td>1.714789</td>\n",
       "      <td>0.235137</td>\n",
       "      <td>0.427175</td>\n",
       "      <td>-1.943878</td>\n",
       "      <td>0.141873</td>\n",
       "      <td>0.475480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.725638</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>-0.828401</td>\n",
       "      <td>-0.168590</td>\n",
       "      <td>0.047003</td>\n",
       "      <td>-0.256251</td>\n",
       "      <td>-0.571609</td>\n",
       "      <td>0.089265</td>\n",
       "      <td>0.919742</td>\n",
       "      <td>0.646862</td>\n",
       "      <td>1.152600</td>\n",
       "      <td>-0.278646</td>\n",
       "      <td>-0.190790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.944091</td>\n",
       "      <td>3.220617</td>\n",
       "      <td>-0.011147</td>\n",
       "      <td>0.412146</td>\n",
       "      <td>6.715380</td>\n",
       "      <td>-1.725246</td>\n",
       "      <td>2.060730</td>\n",
       "      <td>1.686238</td>\n",
       "      <td>0.696162</td>\n",
       "      <td>0.697638</td>\n",
       "      <td>-0.197327</td>\n",
       "      <td>0.494697</td>\n",
       "      <td>0.326459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>-1.232009</td>\n",
       "      <td>0.306020</td>\n",
       "      <td>-1.686558</td>\n",
       "      <td>0.743732</td>\n",
       "      <td>0.178276</td>\n",
       "      <td>-1.958307</td>\n",
       "      <td>-1.010054</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>0.292459</td>\n",
       "      <td>-0.513688</td>\n",
       "      <td>0.835322</td>\n",
       "      <td>-0.669394</td>\n",
       "      <td>-0.649458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>-0.769544</td>\n",
       "      <td>0.057371</td>\n",
       "      <td>0.933577</td>\n",
       "      <td>-0.661311</td>\n",
       "      <td>0.224153</td>\n",
       "      <td>-0.087678</td>\n",
       "      <td>-0.478583</td>\n",
       "      <td>-1.085852</td>\n",
       "      <td>1.241429</td>\n",
       "      <td>1.586037</td>\n",
       "      <td>0.572889</td>\n",
       "      <td>-0.225831</td>\n",
       "      <td>0.861978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.053307</td>\n",
       "      <td>-0.520964</td>\n",
       "      <td>-0.578466</td>\n",
       "      <td>-0.038520</td>\n",
       "      <td>-0.646167</td>\n",
       "      <td>-0.708205</td>\n",
       "      <td>0.093579</td>\n",
       "      <td>1.567240</td>\n",
       "      <td>-0.445872</td>\n",
       "      <td>-0.384570</td>\n",
       "      <td>0.864616</td>\n",
       "      <td>-0.225588</td>\n",
       "      <td>-0.647296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>1.432705</td>\n",
       "      <td>-0.568338</td>\n",
       "      <td>0.080970</td>\n",
       "      <td>-0.138245</td>\n",
       "      <td>3.869644</td>\n",
       "      <td>0.864978</td>\n",
       "      <td>0.203564</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>-0.338967</td>\n",
       "      <td>-0.303467</td>\n",
       "      <td>-0.242296</td>\n",
       "      <td>0.094915</td>\n",
       "      <td>0.474001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>-0.745274</td>\n",
       "      <td>-0.028938</td>\n",
       "      <td>-0.277603</td>\n",
       "      <td>-0.938707</td>\n",
       "      <td>0.054226</td>\n",
       "      <td>-0.635744</td>\n",
       "      <td>-0.015857</td>\n",
       "      <td>-0.102289</td>\n",
       "      <td>0.823431</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.205539</td>\n",
       "      <td>-0.311005</td>\n",
       "      <td>0.210290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>-0.055075</td>\n",
       "      <td>-0.113322</td>\n",
       "      <td>-1.025686</td>\n",
       "      <td>0.491396</td>\n",
       "      <td>-0.674822</td>\n",
       "      <td>-1.648724</td>\n",
       "      <td>2.620455</td>\n",
       "      <td>-0.030234</td>\n",
       "      <td>-1.802748</td>\n",
       "      <td>0.965518</td>\n",
       "      <td>-2.123488</td>\n",
       "      <td>-0.813264</td>\n",
       "      <td>-0.999087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1.341204</td>\n",
       "      <td>-1.224549</td>\n",
       "      <td>0.022587</td>\n",
       "      <td>-0.127594</td>\n",
       "      <td>0.381066</td>\n",
       "      <td>-0.950163</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>-0.633843</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>-0.007171</td>\n",
       "      <td>0.304756</td>\n",
       "      <td>-0.515055</td>\n",
       "      <td>-1.101154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>-1.081127</td>\n",
       "      <td>0.284585</td>\n",
       "      <td>-0.029754</td>\n",
       "      <td>-1.477303</td>\n",
       "      <td>0.362155</td>\n",
       "      <td>-0.542538</td>\n",
       "      <td>-0.341556</td>\n",
       "      <td>-0.759858</td>\n",
       "      <td>1.357990</td>\n",
       "      <td>1.073829</td>\n",
       "      <td>0.241502</td>\n",
       "      <td>-0.298732</td>\n",
       "      <td>1.814652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1.917618</td>\n",
       "      <td>3.083110</td>\n",
       "      <td>-0.026130</td>\n",
       "      <td>0.336896</td>\n",
       "      <td>0.191708</td>\n",
       "      <td>1.046975</td>\n",
       "      <td>-1.067334</td>\n",
       "      <td>0.073369</td>\n",
       "      <td>0.061678</td>\n",
       "      <td>-0.122507</td>\n",
       "      <td>-2.320833</td>\n",
       "      <td>0.065017</td>\n",
       "      <td>-0.087655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>-0.943718</td>\n",
       "      <td>0.089435</td>\n",
       "      <td>1.055180</td>\n",
       "      <td>0.231361</td>\n",
       "      <td>0.245461</td>\n",
       "      <td>0.226678</td>\n",
       "      <td>0.361266</td>\n",
       "      <td>-0.965515</td>\n",
       "      <td>-0.092717</td>\n",
       "      <td>-1.648120</td>\n",
       "      <td>0.346786</td>\n",
       "      <td>-0.257553</td>\n",
       "      <td>-0.356610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>-0.451828</td>\n",
       "      <td>-0.226744</td>\n",
       "      <td>1.069412</td>\n",
       "      <td>-1.633190</td>\n",
       "      <td>-0.023538</td>\n",
       "      <td>0.334185</td>\n",
       "      <td>0.113803</td>\n",
       "      <td>0.922561</td>\n",
       "      <td>0.332645</td>\n",
       "      <td>0.511989</td>\n",
       "      <td>0.653025</td>\n",
       "      <td>0.135653</td>\n",
       "      <td>-0.424830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>-1.229351</td>\n",
       "      <td>0.339765</td>\n",
       "      <td>-1.589406</td>\n",
       "      <td>0.612354</td>\n",
       "      <td>0.169645</td>\n",
       "      <td>-2.038777</td>\n",
       "      <td>-0.909218</td>\n",
       "      <td>-0.022986</td>\n",
       "      <td>0.256235</td>\n",
       "      <td>-0.742080</td>\n",
       "      <td>-0.353976</td>\n",
       "      <td>-0.712598</td>\n",
       "      <td>-0.553699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1.325440</td>\n",
       "      <td>-1.388607</td>\n",
       "      <td>-0.261335</td>\n",
       "      <td>0.259238</td>\n",
       "      <td>1.008627</td>\n",
       "      <td>-1.245808</td>\n",
       "      <td>0.330217</td>\n",
       "      <td>-0.447107</td>\n",
       "      <td>0.090687</td>\n",
       "      <td>-0.297344</td>\n",
       "      <td>-1.951701</td>\n",
       "      <td>-0.630887</td>\n",
       "      <td>-0.167405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>-0.862539</td>\n",
       "      <td>0.107514</td>\n",
       "      <td>-0.161533</td>\n",
       "      <td>0.416875</td>\n",
       "      <td>0.231116</td>\n",
       "      <td>-0.318603</td>\n",
       "      <td>-0.779385</td>\n",
       "      <td>-0.084422</td>\n",
       "      <td>-3.030766</td>\n",
       "      <td>-1.574044</td>\n",
       "      <td>2.642999</td>\n",
       "      <td>-0.436635</td>\n",
       "      <td>1.462342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1.324139</td>\n",
       "      <td>-1.315316</td>\n",
       "      <td>0.203130</td>\n",
       "      <td>-0.399848</td>\n",
       "      <td>-0.795696</td>\n",
       "      <td>-0.622838</td>\n",
       "      <td>-0.178703</td>\n",
       "      <td>-0.979616</td>\n",
       "      <td>0.031232</td>\n",
       "      <td>0.209349</td>\n",
       "      <td>0.521395</td>\n",
       "      <td>-0.584502</td>\n",
       "      <td>2.062147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>-0.160398</td>\n",
       "      <td>-0.339631</td>\n",
       "      <td>1.703937</td>\n",
       "      <td>0.246093</td>\n",
       "      <td>-0.397896</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.061063</td>\n",
       "      <td>1.368559</td>\n",
       "      <td>-1.257393</td>\n",
       "      <td>0.439087</td>\n",
       "      <td>-0.011709</td>\n",
       "      <td>-0.035208</td>\n",
       "      <td>-0.416050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.057301</td>\n",
       "      <td>-0.196862</td>\n",
       "      <td>0.829021</td>\n",
       "      <td>-1.937920</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>-0.137875</td>\n",
       "      <td>-0.581508</td>\n",
       "      <td>2.975434</td>\n",
       "      <td>-1.055595</td>\n",
       "      <td>0.557735</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.268639</td>\n",
       "      <td>-0.127346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>-0.978439</td>\n",
       "      <td>0.163443</td>\n",
       "      <td>-0.212725</td>\n",
       "      <td>0.489275</td>\n",
       "      <td>0.182439</td>\n",
       "      <td>-0.284891</td>\n",
       "      <td>-0.421324</td>\n",
       "      <td>-0.392090</td>\n",
       "      <td>-0.133627</td>\n",
       "      <td>2.138391</td>\n",
       "      <td>0.745538</td>\n",
       "      <td>-0.374072</td>\n",
       "      <td>-0.606420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>-0.624002</td>\n",
       "      <td>-0.102098</td>\n",
       "      <td>-0.925545</td>\n",
       "      <td>0.041010</td>\n",
       "      <td>-0.046906</td>\n",
       "      <td>-1.334314</td>\n",
       "      <td>-0.767389</td>\n",
       "      <td>-0.253456</td>\n",
       "      <td>-0.421357</td>\n",
       "      <td>-0.003786</td>\n",
       "      <td>0.274113</td>\n",
       "      <td>-0.618727</td>\n",
       "      <td>-0.779988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>1.946965</td>\n",
       "      <td>3.381152</td>\n",
       "      <td>0.423799</td>\n",
       "      <td>-0.267636</td>\n",
       "      <td>-1.254834</td>\n",
       "      <td>0.639363</td>\n",
       "      <td>-0.961160</td>\n",
       "      <td>-0.395918</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>-0.369560</td>\n",
       "      <td>-0.931779</td>\n",
       "      <td>0.011320</td>\n",
       "      <td>-1.139365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-1.086909</td>\n",
       "      <td>0.300854</td>\n",
       "      <td>0.247106</td>\n",
       "      <td>-0.316727</td>\n",
       "      <td>0.455830</td>\n",
       "      <td>0.931749</td>\n",
       "      <td>-0.181029</td>\n",
       "      <td>-0.758782</td>\n",
       "      <td>0.257015</td>\n",
       "      <td>-0.862618</td>\n",
       "      <td>0.660463</td>\n",
       "      <td>-0.154142</td>\n",
       "      <td>-0.071434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>-1.089524</td>\n",
       "      <td>0.223254</td>\n",
       "      <td>-0.188838</td>\n",
       "      <td>0.294791</td>\n",
       "      <td>0.242859</td>\n",
       "      <td>-0.351267</td>\n",
       "      <td>0.141172</td>\n",
       "      <td>-0.764611</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>-0.814245</td>\n",
       "      <td>0.137077</td>\n",
       "      <td>-0.403975</td>\n",
       "      <td>-0.395869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>-0.671117</td>\n",
       "      <td>-0.053777</td>\n",
       "      <td>1.205542</td>\n",
       "      <td>-0.193422</td>\n",
       "      <td>0.109789</td>\n",
       "      <td>1.204791</td>\n",
       "      <td>0.198518</td>\n",
       "      <td>0.865851</td>\n",
       "      <td>-0.461784</td>\n",
       "      <td>0.502419</td>\n",
       "      <td>0.774767</td>\n",
       "      <td>0.177741</td>\n",
       "      <td>-0.032378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>-0.730955</td>\n",
       "      <td>-0.035765</td>\n",
       "      <td>-0.418148</td>\n",
       "      <td>-0.770584</td>\n",
       "      <td>0.144462</td>\n",
       "      <td>1.217634</td>\n",
       "      <td>0.590863</td>\n",
       "      <td>-0.598410</td>\n",
       "      <td>-1.276997</td>\n",
       "      <td>2.911405</td>\n",
       "      <td>0.150930</td>\n",
       "      <td>-0.285752</td>\n",
       "      <td>-1.797761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1.329252</td>\n",
       "      <td>-1.387059</td>\n",
       "      <td>-0.371704</td>\n",
       "      <td>0.392805</td>\n",
       "      <td>1.903217</td>\n",
       "      <td>1.175357</td>\n",
       "      <td>-0.727952</td>\n",
       "      <td>-0.072868</td>\n",
       "      <td>-0.369057</td>\n",
       "      <td>-0.225423</td>\n",
       "      <td>1.752527</td>\n",
       "      <td>-0.130417</td>\n",
       "      <td>0.071222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>-0.640238</td>\n",
       "      <td>0.147812</td>\n",
       "      <td>1.086126</td>\n",
       "      <td>-2.779055</td>\n",
       "      <td>0.167292</td>\n",
       "      <td>0.778347</td>\n",
       "      <td>1.077273</td>\n",
       "      <td>-0.321711</td>\n",
       "      <td>0.115314</td>\n",
       "      <td>-0.232130</td>\n",
       "      <td>-0.278421</td>\n",
       "      <td>0.030021</td>\n",
       "      <td>-0.669895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.192572</td>\n",
       "      <td>-0.266334</td>\n",
       "      <td>-0.812869</td>\n",
       "      <td>0.126964</td>\n",
       "      <td>-0.400416</td>\n",
       "      <td>-0.109875</td>\n",
       "      <td>-0.476407</td>\n",
       "      <td>1.351882</td>\n",
       "      <td>1.231955</td>\n",
       "      <td>-1.722860</td>\n",
       "      <td>0.011202</td>\n",
       "      <td>-0.123922</td>\n",
       "      <td>-0.505272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>-1.081630</td>\n",
       "      <td>0.267286</td>\n",
       "      <td>-0.281622</td>\n",
       "      <td>0.418306</td>\n",
       "      <td>0.344094</td>\n",
       "      <td>0.605098</td>\n",
       "      <td>-0.377830</td>\n",
       "      <td>-0.668960</td>\n",
       "      <td>0.390778</td>\n",
       "      <td>-0.828213</td>\n",
       "      <td>0.635444</td>\n",
       "      <td>4.107754</td>\n",
       "      <td>-0.193993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1.487054</td>\n",
       "      <td>-0.114793</td>\n",
       "      <td>0.116395</td>\n",
       "      <td>-0.155696</td>\n",
       "      <td>-0.110424</td>\n",
       "      <td>-0.805598</td>\n",
       "      <td>0.005815</td>\n",
       "      <td>-0.607363</td>\n",
       "      <td>0.161476</td>\n",
       "      <td>-0.062005</td>\n",
       "      <td>1.421890</td>\n",
       "      <td>-0.372638</td>\n",
       "      <td>-1.184008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>-0.752305</td>\n",
       "      <td>-0.028203</td>\n",
       "      <td>0.108205</td>\n",
       "      <td>-1.477278</td>\n",
       "      <td>0.135935</td>\n",
       "      <td>-0.443969</td>\n",
       "      <td>0.155242</td>\n",
       "      <td>-0.181413</td>\n",
       "      <td>0.703573</td>\n",
       "      <td>-0.178459</td>\n",
       "      <td>0.066556</td>\n",
       "      <td>-0.243927</td>\n",
       "      <td>0.513825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0   -0.779282  0.046682 -1.372281  0.526056 -0.098318 -0.518909  0.701723   \n",
       "1   -1.143789  0.226200  0.524269 -0.700368  0.429386 -0.554615 -0.340130   \n",
       "2   -0.207060 -0.351878 -0.659215 -0.086648 -0.453202 -0.790520 -0.056720   \n",
       "3   -0.759802  0.012021  0.801030 -2.440905  0.227237 -0.040985  0.409850   \n",
       "4   -1.192579  0.779302 -1.818313  0.794719  0.429588  3.154097  1.839061   \n",
       "5   -1.228604  0.426378 -0.979864 -0.247193  0.459192 -0.066192 -1.448980   \n",
       "6   -0.684186 -0.017125 -0.633368 -0.390375 -0.004051 -0.724386 -0.940293   \n",
       "7   -0.832640  0.042129 -0.555234 -0.578430  0.002527 -1.195204 -0.396563   \n",
       "8   -0.525737 -0.050751  1.643198 -1.720299  0.040105 -0.153797  0.232904   \n",
       "9    0.135853  0.079028 -0.702650  0.183677 -0.684583  0.049694 -0.398385   \n",
       "10  -0.823014  0.072077 -0.853458 -0.163122 -0.010287 -1.006865 -0.681461   \n",
       "11  -1.094559  0.201951 -0.257788  1.426358  0.237420 -1.187836 -1.123423   \n",
       "12  -0.532247 -0.143635  1.335205 -1.305557  0.080791  0.128988 -0.074948   \n",
       "13   0.032189 -0.364214  0.984574 -2.175333 -0.364840  0.176266 -0.595259   \n",
       "14  -0.970383  0.111723  2.256356  1.321040  0.183366 -0.190510  0.615912   \n",
       "15  -0.802024  0.010558 -0.519852 -0.606740 -0.001044 -1.135421 -0.529680   \n",
       "16   0.050000 -0.483395 -0.117858 -0.686213 -0.485919  0.258766 -0.056492   \n",
       "17   0.056649 -0.550090 -0.949781  0.466610 -0.554758  0.164951 -0.562458   \n",
       "18  -1.088007  0.197458 -0.273877 -1.154001  0.424371 -0.010448 -0.725374   \n",
       "19   1.327613 -1.388404 -0.388812  0.388301  0.430668  2.602374 -1.845792   \n",
       "20  -0.176383 -0.402101 -0.406060 -0.431329 -0.300219  0.251357 -0.253567   \n",
       "21   1.307242 -1.388448  0.623496 -0.991475 -0.245434 -0.244169  0.040102   \n",
       "22   1.887831  2.890825  0.120417  0.112988 -0.748978  0.487974 -0.931154   \n",
       "23   1.324486 -1.389708 -0.293408  0.284724 -0.142051 -0.198274 -0.538138   \n",
       "24   1.354292 -1.157860 -0.238734  0.230908 -0.238265 -0.273054 -0.496439   \n",
       "25  -0.794372  0.043294  0.341218 -1.828265  0.292837  0.393407 -0.381963   \n",
       "26  -0.963710  0.126650 -0.173215 -1.211778  0.366479  0.521503 -0.821885   \n",
       "27   1.335881 -1.378706 -0.142699  0.172779  9.548751 -2.106839  3.086271   \n",
       "28  -0.725638  0.040122 -0.828401 -0.168590  0.047003 -0.256251 -0.571609   \n",
       "29   1.944091  3.220617 -0.011147  0.412146  6.715380 -1.725246  2.060730   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "324 -1.232009  0.306020 -1.686558  0.743732  0.178276 -1.958307 -1.010054   \n",
       "325 -0.769544  0.057371  0.933577 -0.661311  0.224153 -0.087678 -0.478583   \n",
       "326  0.053307 -0.520964 -0.578466 -0.038520 -0.646167 -0.708205  0.093579   \n",
       "327  1.432705 -0.568338  0.080970 -0.138245  3.869644  0.864978  0.203564   \n",
       "328 -0.745274 -0.028938 -0.277603 -0.938707  0.054226 -0.635744 -0.015857   \n",
       "329 -0.055075 -0.113322 -1.025686  0.491396 -0.674822 -1.648724  2.620455   \n",
       "330  1.341204 -1.224549  0.022587 -0.127594  0.381066 -0.950163  0.185944   \n",
       "331 -1.081127  0.284585 -0.029754 -1.477303  0.362155 -0.542538 -0.341556   \n",
       "332  1.917618  3.083110 -0.026130  0.336896  0.191708  1.046975 -1.067334   \n",
       "333 -0.943718  0.089435  1.055180  0.231361  0.245461  0.226678  0.361266   \n",
       "334 -0.451828 -0.226744  1.069412 -1.633190 -0.023538  0.334185  0.113803   \n",
       "335 -1.229351  0.339765 -1.589406  0.612354  0.169645 -2.038777 -0.909218   \n",
       "336  1.325440 -1.388607 -0.261335  0.259238  1.008627 -1.245808  0.330217   \n",
       "337 -0.862539  0.107514 -0.161533  0.416875  0.231116 -0.318603 -0.779385   \n",
       "338  1.324139 -1.315316  0.203130 -0.399848 -0.795696 -0.622838 -0.178703   \n",
       "339 -0.160398 -0.339631  1.703937  0.246093 -0.397896 -0.496996  0.061063   \n",
       "340  0.057301 -0.196862  0.829021 -1.937920 -0.458902 -0.137875 -0.581508   \n",
       "341 -0.978439  0.163443 -0.212725  0.489275  0.182439 -0.284891 -0.421324   \n",
       "342 -0.624002 -0.102098 -0.925545  0.041010 -0.046906 -1.334314 -0.767389   \n",
       "343  1.946965  3.381152  0.423799 -0.267636 -1.254834  0.639363 -0.961160   \n",
       "344 -1.086909  0.300854  0.247106 -0.316727  0.455830  0.931749 -0.181029   \n",
       "345 -1.089524  0.223254 -0.188838  0.294791  0.242859 -0.351267  0.141172   \n",
       "346 -0.671117 -0.053777  1.205542 -0.193422  0.109789  1.204791  0.198518   \n",
       "347 -0.730955 -0.035765 -0.418148 -0.770584  0.144462  1.217634  0.590863   \n",
       "348  1.329252 -1.387059 -0.371704  0.392805  1.903217  1.175357 -0.727952   \n",
       "349 -0.640238  0.147812  1.086126 -2.779055  0.167292  0.778347  1.077273   \n",
       "350 -0.192572 -0.266334 -0.812869  0.126964 -0.400416 -0.109875 -0.476407   \n",
       "351 -1.081630  0.267286 -0.281622  0.418306  0.344094  0.605098 -0.377830   \n",
       "352  1.487054 -0.114793  0.116395 -0.155696 -0.110424 -0.805598  0.005815   \n",
       "353 -0.752305 -0.028203  0.108205 -1.477278  0.135935 -0.443969  0.155242   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0   -1.025190 -0.817553  1.193480  0.289122  3.665580 -0.479830  \n",
       "1   -0.950101 -1.788952 -0.816782  0.122835 -0.479731 -0.729605  \n",
       "2    1.268348  1.413974 -1.203726 -0.758255 -0.223999 -0.164471  \n",
       "3   -0.319775  0.528061 -0.206056 -0.146037 -0.112363  1.137874  \n",
       "4   -3.204694  0.195145  0.103449 -0.628099 -0.540744 -0.996219  \n",
       "5   -0.016962 -0.253414 -0.957044  0.360711 -0.347894 -0.141433  \n",
       "6    0.685945 -1.072874 -1.024015  1.553152 -0.324153 -0.547408  \n",
       "7    1.114157 -0.122953 -0.906826 -0.867879 -0.323447 -0.036915  \n",
       "8   -0.324561  0.808827  2.192158 -1.191256 -0.089669  0.977007  \n",
       "9    1.708434 -0.563914 -0.600616  0.373670 -0.109243 -0.844511  \n",
       "10   1.204574 -0.113234 -0.935417  0.390315 -0.286094 -0.394194  \n",
       "11  -1.294700  0.594517 -1.796983 -0.944997 -0.699057 -0.328123  \n",
       "12  -0.255016  0.676137  1.453431 -0.877885 -0.094830  0.484352  \n",
       "13   2.924795 -1.264507 -0.211471  0.523410  0.323176 -0.212280  \n",
       "14  -0.924141  0.926155 -2.267388 -0.052985 -0.161320  0.619036  \n",
       "15   1.286244 -0.042890 -0.967516 -1.702434 -0.308814 -0.743266  \n",
       "16   1.519008 -0.777113 -0.885896  0.855188 -0.045535 -0.381771  \n",
       "17   1.721568 -0.590728 -0.352252 -0.103668 -0.195062 -0.935088  \n",
       "18  -0.695716  1.137542  0.121476  0.845598 -0.256113  1.300919  \n",
       "19  -0.406840 -0.737346 -0.781541  0.336719 -0.168536 -0.715806  \n",
       "20   0.971402  0.138848 -0.905731  0.656374 -0.123651  0.668999  \n",
       "21  -0.944423 -0.117694  0.232065  0.018051 -0.454907  0.281327  \n",
       "22  -0.261225  0.161104 -0.117257 -1.707151 -0.080691  1.174269  \n",
       "23  -0.667592  0.009535  0.406817 -0.222981 -0.555188  0.629739  \n",
       "24  -0.682741  0.014758  0.246315 -0.246031  3.826799  2.059421  \n",
       "25  -0.091684  0.104467 -1.342048  0.543630 -0.093569  0.582312  \n",
       "26   0.342614 -0.061456 -0.530902  0.169190 -0.101348 -0.349721  \n",
       "27   1.714789  0.235137  0.427175 -1.943878  0.141873  0.475480  \n",
       "28   0.089265  0.919742  0.646862  1.152600 -0.278646 -0.190790  \n",
       "29   1.686238  0.696162  0.697638 -0.197327  0.494697  0.326459  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "324  0.012286  0.292459 -0.513688  0.835322 -0.669394 -0.649458  \n",
       "325 -1.085852  1.241429  1.586037  0.572889 -0.225831  0.861978  \n",
       "326  1.567240 -0.445872 -0.384570  0.864616 -0.225588 -0.647296  \n",
       "327  0.422200 -0.338967 -0.303467 -0.242296  0.094915  0.474001  \n",
       "328 -0.102289  0.823431  0.019600  0.205539 -0.311005  0.210290  \n",
       "329 -0.030234 -1.802748  0.965518 -2.123488 -0.813264 -0.999087  \n",
       "330 -0.633843  0.082300 -0.007171  0.304756 -0.515055 -1.101154  \n",
       "331 -0.759858  1.357990  1.073829  0.241502 -0.298732  1.814652  \n",
       "332  0.073369  0.061678 -0.122507 -2.320833  0.065017 -0.087655  \n",
       "333 -0.965515 -0.092717 -1.648120  0.346786 -0.257553 -0.356610  \n",
       "334  0.922561  0.332645  0.511989  0.653025  0.135653 -0.424830  \n",
       "335 -0.022986  0.256235 -0.742080 -0.353976 -0.712598 -0.553699  \n",
       "336 -0.447107  0.090687 -0.297344 -1.951701 -0.630887 -0.167405  \n",
       "337 -0.084422 -3.030766 -1.574044  2.642999 -0.436635  1.462342  \n",
       "338 -0.979616  0.031232  0.209349  0.521395 -0.584502  2.062147  \n",
       "339  1.368559 -1.257393  0.439087 -0.011709 -0.035208 -0.416050  \n",
       "340  2.975434 -1.055595  0.557735  0.037657  0.268639 -0.127346  \n",
       "341 -0.392090 -0.133627  2.138391  0.745538 -0.374072 -0.606420  \n",
       "342 -0.253456 -0.421357 -0.003786  0.274113 -0.618727 -0.779988  \n",
       "343 -0.395918  0.119700 -0.369560 -0.931779  0.011320 -1.139365  \n",
       "344 -0.758782  0.257015 -0.862618  0.660463 -0.154142 -0.071434  \n",
       "345 -0.764611  0.578089 -0.814245  0.137077 -0.403975 -0.395869  \n",
       "346  0.865851 -0.461784  0.502419  0.774767  0.177741 -0.032378  \n",
       "347 -0.598410 -1.276997  2.911405  0.150930 -0.285752 -1.797761  \n",
       "348 -0.072868 -0.369057 -0.225423  1.752527 -0.130417  0.071222  \n",
       "349 -0.321711  0.115314 -0.232130 -0.278421  0.030021 -0.669895  \n",
       "350  1.351882  1.231955 -1.722860  0.011202 -0.123922 -0.505272  \n",
       "351 -0.668960  0.390778 -0.828213  0.635444  4.107754 -0.193993  \n",
       "352 -0.607363  0.161476 -0.062005  1.421890 -0.372638 -1.184008  \n",
       "353 -0.181413  0.703573 -0.178459  0.066556 -0.243927  0.513825  \n",
       "\n",
       "[354 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = dec.PCA(n_components=13, whiten=True)\n",
    "pca.fit(x_train)\n",
    "x_train_white = pd.DataFrame(pca.transform(x_train), columns=x.columns)\n",
    "x_train_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = kr.models.Sequential()\n",
    "\n",
    "m.add(kr.layers.Dense(20, input_dim=13, activation=\"relu\"))\n",
    "m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "#m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "m.compile(loss=\"mean_squared_logarithmic_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "354/354 [==============================] - 0s 565us/step - loss: 2.8666\n",
      "Epoch 2/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 1.1722\n",
      "Epoch 3/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.7981\n",
      "Epoch 4/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.7294\n",
      "Epoch 5/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.7051\n",
      "Epoch 6/100\n",
      "354/354 [==============================] - 0s 229us/step - loss: 0.6937\n",
      "Epoch 7/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.6937\n",
      "Epoch 8/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.6926\n",
      "Epoch 9/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.6696\n",
      "Epoch 10/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.6385\n",
      "Epoch 11/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.6410\n",
      "Epoch 12/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.4246\n",
      "Epoch 13/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.2199\n",
      "Epoch 14/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1327\n",
      "Epoch 15/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1545\n",
      "Epoch 16/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1141\n",
      "Epoch 17/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0955\n",
      "Epoch 18/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0866\n",
      "Epoch 19/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0861\n",
      "Epoch 20/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0835\n",
      "Epoch 21/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0815\n",
      "Epoch 22/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0771\n",
      "Epoch 23/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0777\n",
      "Epoch 24/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0731\n",
      "Epoch 25/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0733\n",
      "Epoch 26/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0700\n",
      "Epoch 27/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0714\n",
      "Epoch 28/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0689\n",
      "Epoch 29/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0662\n",
      "Epoch 30/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0659\n",
      "Epoch 31/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0648\n",
      "Epoch 32/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0660\n",
      "Epoch 33/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0705\n",
      "Epoch 34/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0605\n",
      "Epoch 35/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0620\n",
      "Epoch 36/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0611\n",
      "Epoch 37/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0636\n",
      "Epoch 38/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0630\n",
      "Epoch 39/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0583\n",
      "Epoch 40/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0573\n",
      "Epoch 41/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0609\n",
      "Epoch 42/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0647\n",
      "Epoch 43/100\n",
      "354/354 [==============================] - 0s 203us/step - loss: 0.0579\n",
      "Epoch 44/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0651\n",
      "Epoch 45/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0593\n",
      "Epoch 46/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0593\n",
      "Epoch 47/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0574\n",
      "Epoch 48/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0549\n",
      "Epoch 49/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0551\n",
      "Epoch 50/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0541\n",
      "Epoch 51/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0536\n",
      "Epoch 52/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0524\n",
      "Epoch 53/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0541\n",
      "Epoch 54/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0521\n",
      "Epoch 55/100\n",
      "354/354 [==============================] - 0s 232us/step - loss: 0.0533\n",
      "Epoch 56/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0542\n",
      "Epoch 57/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0530\n",
      "Epoch 58/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0521\n",
      "Epoch 59/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0572\n",
      "Epoch 60/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0568\n",
      "Epoch 61/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0538\n",
      "Epoch 62/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0522\n",
      "Epoch 63/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0518\n",
      "Epoch 64/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0543\n",
      "Epoch 65/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0508\n",
      "Epoch 66/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0529\n",
      "Epoch 67/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0529\n",
      "Epoch 68/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0527\n",
      "Epoch 69/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0515\n",
      "Epoch 70/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0623\n",
      "Epoch 71/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0531\n",
      "Epoch 72/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0509\n",
      "Epoch 73/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0512\n",
      "Epoch 74/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0560\n",
      "Epoch 75/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0501\n",
      "Epoch 76/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0501\n",
      "Epoch 77/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0513\n",
      "Epoch 78/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0596\n",
      "Epoch 79/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0522\n",
      "Epoch 80/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0519\n",
      "Epoch 81/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0527\n",
      "Epoch 82/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0492\n",
      "Epoch 83/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0614\n",
      "Epoch 84/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0611\n",
      "Epoch 85/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0486\n",
      "Epoch 86/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0526\n",
      "Epoch 87/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0493\n",
      "Epoch 88/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0506\n",
      "Epoch 89/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0519\n",
      "Epoch 90/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0504\n",
      "Epoch 91/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0531\n",
      "Epoch 92/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0477\n",
      "Epoch 93/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0522\n",
      "Epoch 94/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0562\n",
      "Epoch 95/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0553\n",
      "Epoch 96/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0492\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 0s 141us/step - loss: 0.0501\n",
      "Epoch 98/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0516\n",
      "Epoch 99/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0469\n",
      "Epoch 100/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14643828>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(x_train, y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18., 15., 16., 16., 19., 17., 26., 19., 14., 22., 33., 27., 28.,\n",
       "        32., 24., 19., 26., 19., 10., 13., 14., 32., 29., 27., 27., 25.,\n",
       "        18., 25., 22., 22., 22., 22., 22., 15., 31., 18., 23., 27., 26.,\n",
       "        24., 22., 21., 24., 27., 24., 25., 21., 28., 28., 27., 29., 19.,\n",
       "        22., 25., 18., 23., 18., 30., 20., 22., 33., 20., 15., 27., 23.,\n",
       "        17., 23., 19., 30., 22., 27., 23., 24., 16., 30., 25., 23., 31.,\n",
       "        29., 31., 21., 23., 17., 32., 29., 33., 25., 12., 18., 21., 25.,\n",
       "        27., 22., 26., 30., 24., 21., 28., 23., 25., 29., 21., 20., 27.,\n",
       "        15., 23., 27., 21., 32., 25., 19., 26., 31., 19., 21., 23., 19.,\n",
       "        21., 23., 20., 30., 23., 22., 16., 19., 28., 27., 19., 30., 20.,\n",
       "        28., 20., 20., 17., 21., 24., 26., 13., 22., 38., 23., 32., 16.,\n",
       "        24., 24., 21., 22., 23., 21., 14., 24., 33.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(x_test).round().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27.5, 17.8, 12.7, 12. , 15.6,  8.3, 23.9, 16.1, 11.8, 20.4, 31.6,\n",
       "       22.6, 23.7, 42.3, 24. , 13.8, 29. ,  8.5, 13.2, 15.6, 14.6, 48.8,\n",
       "       50. , 24.8, 23.4, 22. , 12.8, 28.7, 17. , 22.7, 20. , 19.9, 21.2,\n",
       "        6.3, 29. , 13.4, 19.9, 25.1, 22.2, 20.3, 22. , 20.5, 30.3, 25. ,\n",
       "       25. , 21.6, 18.2, 33.4, 32. , 23.1, 24.1, 15. , 17.7, 19. ,  9.6,\n",
       "       22.9, 16.6, 37.6, 20. , 20.8, 37.3, 12.7, 14.9, 34.7, 13.6,  9.5,\n",
       "       27.5, 15.2, 35.2, 18.2, 23.9, 20.9, 23.8,  7.5, 44.8, 22.6, 16.8,\n",
       "       32. , 48.3, 50. , 19.6, 21.8,  8.4, 33.1, 30.1, 50. , 23.3, 14.8,\n",
       "       14.1, 24.5, 41.3, 24.6, 15.3, 23. , 36.5, 27.1, 17.8, 26.6, 23.1,\n",
       "       18.9, 23.7, 19.1, 17.5, 50. ,  5.6, 28.4, 36.2, 19.5, 43.5, 25. ,\n",
       "        7. , 18.7, 50. , 22.5, 19.9, 26.5, 14.1, 20. , 24.3, 20.1, 36. ,\n",
       "       21.7, 21.8, 20.2, 17.6, 50. , 29.9, 18.9, 33.3, 16.7, 50. , 12.5,\n",
       "       19.4, 13.4, 19.8, 26.6, 23.9,  8.8, 18.9, 50. , 21.7, 36.1, 17.2,\n",
       "       18.2, 20.3, 21.7, 21.4, 20.6, 13.8, 13.4, 23.1, 44. ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.as_matrix().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 0s 724us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07629462704062462"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=array([-22.77577605,  -5.53794422, -12.50693518, -27.70925837,\n",
       "       -27.16708355, -20.23549301,  19.71647046, -22.81405217,\n",
       "       -13.27528933,  27.94483704,  -6.25947823,  43.9926774 ,\n",
       "       -11.2875507 ]), pvalue=array([1.57290413e-067, 6.65842010e-008, 3.32739670e-029, 6.15484129e-085,\n",
       "       4.37837560e-083, 3.84965259e-058, 3.38781872e-056, 1.14065890e-067,\n",
       "       5.38733362e-032, 9.75856339e-086, 1.32270946e-009, 2.31153632e-133,\n",
       "       6.91235424e-025]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-a7642948b294>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_oneway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36mf_oneway\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3210\u001b[0m     \u001b[1;31m# ANOVA on N groups, each in its own array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3211\u001b[0m     \u001b[0mnum_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3212\u001b[1;33m     \u001b[0malldata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3213\u001b[0m     \u001b[0mbign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "stats.f_oneway(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = mod.train_test_split(x, y, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "m.add(kr.layers.Dense(20, input_dim=13, activation=\"relu\"))\n",
    "m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "m.compile(loss=\"mean_squared_logarithmic_error\", optimizer=\"adam\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "354/354 [==============================] - 0s 791us/step - loss: 1.5852\n",
      "Epoch 2/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.2164\n",
      "Epoch 3/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1449\n",
      "Epoch 4/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1322\n",
      "Epoch 5/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1236\n",
      "Epoch 6/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1201\n",
      "Epoch 7/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1153\n",
      "Epoch 8/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1084\n",
      "Epoch 9/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1019\n",
      "Epoch 10/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0979\n",
      "Epoch 11/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0941\n",
      "Epoch 12/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0978\n",
      "Epoch 13/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0888\n",
      "Epoch 14/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0904\n",
      "Epoch 15/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0857\n",
      "Epoch 16/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0899\n",
      "Epoch 17/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0840\n",
      "Epoch 18/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0785\n",
      "Epoch 19/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0782\n",
      "Epoch 20/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0774\n",
      "Epoch 21/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0798\n",
      "Epoch 22/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0860\n",
      "Epoch 23/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0761\n",
      "Epoch 24/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0876\n",
      "Epoch 25/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0716\n",
      "Epoch 26/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0742\n",
      "Epoch 27/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0807\n",
      "Epoch 28/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0765\n",
      "Epoch 29/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0760\n",
      "Epoch 30/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0871\n",
      "Epoch 31/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0749\n",
      "Epoch 32/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0810\n",
      "Epoch 33/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0852\n",
      "Epoch 34/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0715\n",
      "Epoch 35/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0693\n",
      "Epoch 36/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0698\n",
      "Epoch 37/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0723\n",
      "Epoch 38/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0738\n",
      "Epoch 39/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0685\n",
      "Epoch 40/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0843\n",
      "Epoch 41/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0698\n",
      "Epoch 42/100\n",
      "354/354 [==============================] - 0s 401us/step - loss: 0.0638\n",
      "Epoch 43/100\n",
      "354/354 [==============================] - 0s 373us/step - loss: 0.0721\n",
      "Epoch 44/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0637\n",
      "Epoch 45/100\n",
      "354/354 [==============================] - 0s 339us/step - loss: 0.0627\n",
      "Epoch 46/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0627\n",
      "Epoch 47/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0640\n",
      "Epoch 48/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0648\n",
      "Epoch 49/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0596\n",
      "Epoch 50/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0604\n",
      "Epoch 51/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0651\n",
      "Epoch 52/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0618\n",
      "Epoch 53/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0659\n",
      "Epoch 54/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0737\n",
      "Epoch 55/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0695\n",
      "Epoch 56/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0675\n",
      "Epoch 57/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0617\n",
      "Epoch 58/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0572\n",
      "Epoch 59/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0575\n",
      "Epoch 60/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0625\n",
      "Epoch 61/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0760\n",
      "Epoch 62/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0651\n",
      "Epoch 63/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0609\n",
      "Epoch 64/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0589\n",
      "Epoch 65/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0618\n",
      "Epoch 66/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0895\n",
      "Epoch 67/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0777\n",
      "Epoch 68/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0633\n",
      "Epoch 69/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0567\n",
      "Epoch 70/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0657\n",
      "Epoch 71/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.1143\n",
      "Epoch 72/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0623\n",
      "Epoch 73/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0586\n",
      "Epoch 74/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0592\n",
      "Epoch 75/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0544\n",
      "Epoch 76/100\n",
      "354/354 [==============================] - 0s 339us/step - loss: 0.0582\n",
      "Epoch 77/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0570\n",
      "Epoch 78/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0527\n",
      "Epoch 79/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0556\n",
      "Epoch 80/100\n",
      "354/354 [==============================] - 0s 229us/step - loss: 0.0544\n",
      "Epoch 81/100\n",
      "354/354 [==============================] - 0s 246us/step - loss: 0.0545\n",
      "Epoch 82/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0581\n",
      "Epoch 83/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0534\n",
      "Epoch 84/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0514\n",
      "Epoch 85/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0567\n",
      "Epoch 86/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0607\n",
      "Epoch 87/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0555\n",
      "Epoch 88/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0528\n",
      "Epoch 89/100\n",
      "354/354 [==============================] - 0s 424us/step - loss: 0.0543\n",
      "Epoch 90/100\n",
      "354/354 [==============================] - 0s 514us/step - loss: 0.0505\n",
      "Epoch 91/100\n",
      "354/354 [==============================] - 0s 339us/step - loss: 0.0517\n",
      "Epoch 92/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0500\n",
      "Epoch 93/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0524\n",
      "Epoch 94/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0614\n",
      "Epoch 95/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0548\n",
      "Epoch 96/100\n",
      "354/354 [==============================] - 0s 311us/step - loss: 0.0530\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 0s 254us/step - loss: 0.0520\n",
      "Epoch 98/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0497\n",
      "Epoch 99/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0521\n",
      "Epoch 100/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x178cae48>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "m.fit(x_train, y_train, epochs=100, batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27., 22., 22., 21., 24., 16., 20., 23., 24., 23., 19., 14., 26.,\n",
       "        22., 14., 23., 18., 25., 17., 30., 14., 26., 11., 19., 20., 24.,\n",
       "        27., 24., 23., 19., 22., 19., 24., 22., 24., 27., 13., 10., 25.,\n",
       "        20., 17., 23., 28., 26., 23., 18., 27., 27., 15., 20., 27.,  7.,\n",
       "        10., 27., 22., 21., 33., 15., 24., 23., 19., 21., 18., 28., 27.,\n",
       "        22., 24., 32., 31., 14., 26., 26., 26., 25., 14., 24., 16.,  7.,\n",
       "        26., 15., 15., 21., 22., 16., 34., 16., 18., 33., 18., 18.,  4.,\n",
       "        30., 23., 23., 18., 35., 16., 22., 26., 25., 25.,  8., 20., 20.,\n",
       "        24., 19., 25., 18., 21., 27., 24., 28., 26., 23., 18., 23., 27.,\n",
       "        26., 16., 13., 12., 22., 19., 31., 20., 24., 26., 29., 21., 21.,\n",
       "        23., 24., 16., 13., 15., 30., 19., 28., 25., 32., 21., 36., 18.,\n",
       "        30., 23., 26., 18., 26., 29., 15., 11., 26.]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(x_test).round().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([24.8, 18.7, 16.1, 21. , 19.3, 13.4, 15. , 24.5, 16.8, 15.3, 20.3,\n",
       "       13.4, 34.7, 18.7, 13.9, 17.1, 17.4, 21.8, 17.2, 20.7, 19.1, 22.2,\n",
       "        5. , 15.4, 21.4, 17.4, 26.6, 24.5, 22. , 20.8, 21.5, 16.1, 20.9,\n",
       "       19.5, 25. , 33.2,  7.2, 13.8, 20.1, 17.6, 10.9, 18.8, 28.7, 23.2,\n",
       "       22.2, 14.9, 23.7, 28. ,  9.7, 23. , 50. , 13.8, 13.8, 26.7, 18.2,\n",
       "       27.9, 32. ,  8.3, 18.9, 19.4, 11.5, 19.4, 20.4, 22. , 31.7, 21.2,\n",
       "       19.7, 33.1, 26.4, 13.9, 21.1, 23.1, 26.6, 20.3,  8.7, 23.1, 10.8,\n",
       "        7. , 25. , 17.3, 13.6, 17.7, 18.4, 13.1, 39.8, 20.1, 27.5, 50. ,\n",
       "        7.2, 15. , 13.2, 33.1, 28.7, 30.3, 14.4, 43.1, 10.2, 21.2, 20.6,\n",
       "       22.6, 21.7,  7.4, 13.1, 20. , 18.3, 21.2, 24.3, 19.8, 17.2, 22. ,\n",
       "       23.4, 29.1, 23.8, 16.8, 13.4, 23.7, 20.9, 20.4, 13.1, 20.2, 10.4,\n",
       "       22.4, 20.5, 24.5, 17.4, 18.5, 23.3, 23.5, 17. , 21.4, 21.7, 20.2,\n",
       "       15.6, 15.7,  8.3, 29.8, 19.3, 26.2, 22.6, 34.9,  7. , 30.1,  9.6,\n",
       "       31.2, 23.8, 21.9, 22.5, 23.9, 24.8, 15.6, 10.5, 22.8],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_test.as_matrix().astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27., 22., 22., 21., 24., 16., 20., 23., 24., 23., 19., 14., 26.,\n",
       "        22., 14., 23., 18., 25., 17., 30., 14., 26., 11., 19., 20., 24.,\n",
       "        27., 24., 23., 19., 22., 19., 24., 22., 24., 27., 13., 10., 25.,\n",
       "        20., 17., 23., 28., 26., 23., 18., 27., 27., 15., 20., 27.,  7.,\n",
       "        10., 27., 22., 21., 33., 15., 24., 23., 19., 21., 18., 28., 27.,\n",
       "        22., 24., 32., 31., 14., 26., 26., 26., 25., 14., 24., 16.,  7.,\n",
       "        26., 15., 15., 21., 22., 16., 34., 16., 18., 33., 18., 18.,  4.,\n",
       "        30., 23., 23., 18., 35., 16., 22., 26., 25., 25.,  8., 20., 20.,\n",
       "        24., 19., 25., 18., 21., 27., 24., 28., 26., 23., 18., 23., 27.,\n",
       "        26., 16., 13., 12., 22., 19., 31., 20., 24., 26., 29., 21., 21.,\n",
       "        23., 24., 16., 13., 15., 30., 19., 28., 25., 32., 21., 36., 18.,\n",
       "        30., 23., 26., 18., 26., 29., 15., 11., 26.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "m.predict(x_test).round().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([24.8, 18.7, 16.1, 21. , 19.3, 13.4, 15. , 24.5, 16.8, 15.3, 20.3,\n",
       "       13.4, 34.7, 18.7, 13.9, 17.1, 17.4, 21.8, 17.2, 20.7, 19.1, 22.2,\n",
       "        5. , 15.4, 21.4, 17.4, 26.6, 24.5, 22. , 20.8, 21.5, 16.1, 20.9,\n",
       "       19.5, 25. , 33.2,  7.2, 13.8, 20.1, 17.6, 10.9, 18.8, 28.7, 23.2,\n",
       "       22.2, 14.9, 23.7, 28. ,  9.7, 23. , 50. , 13.8, 13.8, 26.7, 18.2,\n",
       "       27.9, 32. ,  8.3, 18.9, 19.4, 11.5, 19.4, 20.4, 22. , 31.7, 21.2,\n",
       "       19.7, 33.1, 26.4, 13.9, 21.1, 23.1, 26.6, 20.3,  8.7, 23.1, 10.8,\n",
       "        7. , 25. , 17.3, 13.6, 17.7, 18.4, 13.1, 39.8, 20.1, 27.5, 50. ,\n",
       "        7.2, 15. , 13.2, 33.1, 28.7, 30.3, 14.4, 43.1, 10.2, 21.2, 20.6,\n",
       "       22.6, 21.7,  7.4, 13.1, 20. , 18.3, 21.2, 24.3, 19.8, 17.2, 22. ,\n",
       "       23.4, 29.1, 23.8, 16.8, 13.4, 23.7, 20.9, 20.4, 13.1, 20.2, 10.4,\n",
       "       22.4, 20.5, 24.5, 17.4, 18.5, 23.3, 23.5, 17. , 21.4, 21.7, 20.2,\n",
       "       15.6, 15.7,  8.3, 29.8, 19.3, 26.2, 22.6, 34.9,  7. , 30.1,  9.6,\n",
       "       31.2, 23.8, 21.9, 22.5, 23.9, 24.8, 15.6, 10.5, 22.8],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_test.as_matrix().astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06881635322382576"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "m.evaluate(x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler = pre.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "m.add(kr.layers.Dense(20, input_dim=13, activation=\"relu\"))\n",
    "m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "m.compile(loss=\"mean_squared_logarithmic_error\", optimizer=\"adam\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "354/354 [==============================] - 0s 311us/step - loss: 5.8913\n",
      "Epoch 2/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 2.5642\n",
      "Epoch 3/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 1.5775\n",
      "Epoch 4/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 1.1344\n",
      "Epoch 5/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.8729\n",
      "Epoch 6/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.6969\n",
      "Epoch 7/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.5729\n",
      "Epoch 8/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.4802\n",
      "Epoch 9/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.4092\n",
      "Epoch 10/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.3543\n",
      "Epoch 11/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.3105\n",
      "Epoch 12/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.2753\n",
      "Epoch 13/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.2467\n",
      "Epoch 14/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.2235\n",
      "Epoch 15/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.2042\n",
      "Epoch 16/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1879\n",
      "Epoch 17/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1742\n",
      "Epoch 18/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1627\n",
      "Epoch 19/100\n",
      "354/354 [==============================] - 0s 288us/step - loss: 0.1526\n",
      "Epoch 20/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1442\n",
      "Epoch 21/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1369\n",
      "Epoch 22/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1303\n",
      "Epoch 23/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.1250\n",
      "Epoch 24/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.1200\n",
      "Epoch 25/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.1156\n",
      "Epoch 26/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1117\n",
      "Epoch 27/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1083\n",
      "Epoch 28/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1049\n",
      "Epoch 29/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1020\n",
      "Epoch 30/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0993\n",
      "Epoch 31/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0969\n",
      "Epoch 32/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0946\n",
      "Epoch 33/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0925\n",
      "Epoch 34/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0905\n",
      "Epoch 35/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0887\n",
      "Epoch 36/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0869\n",
      "Epoch 37/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0852\n",
      "Epoch 38/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0838\n",
      "Epoch 39/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0822\n",
      "Epoch 40/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0808\n",
      "Epoch 41/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0794\n",
      "Epoch 42/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0781\n",
      "Epoch 43/100\n",
      "354/354 [==============================] - ETA: 0s - loss: 0.077 - 0s 226us/step - loss: 0.0768\n",
      "Epoch 44/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0757\n",
      "Epoch 45/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0745\n",
      "Epoch 46/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0734\n",
      "Epoch 47/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0724\n",
      "Epoch 48/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0714\n",
      "Epoch 49/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0704\n",
      "Epoch 50/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0695\n",
      "Epoch 51/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0685\n",
      "Epoch 52/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0677\n",
      "Epoch 53/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0668\n",
      "Epoch 54/100\n",
      "354/354 [==============================] - 0s 412us/step - loss: 0.0660\n",
      "Epoch 55/100\n",
      "354/354 [==============================] - 0s 384us/step - loss: 0.0652\n",
      "Epoch 56/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0643\n",
      "Epoch 57/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0635\n",
      "Epoch 58/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0628\n",
      "Epoch 59/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0621\n",
      "Epoch 60/100\n",
      "354/354 [==============================] - 0s 367us/step - loss: 0.0613\n",
      "Epoch 61/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0607\n",
      "Epoch 62/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0601\n",
      "Epoch 63/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0594\n",
      "Epoch 64/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0588\n",
      "Epoch 65/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0581\n",
      "Epoch 66/100\n",
      "354/354 [==============================] - 0s 395us/step - loss: 0.0575\n",
      "Epoch 67/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0569\n",
      "Epoch 68/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0563\n",
      "Epoch 69/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0557\n",
      "Epoch 70/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0551\n",
      "Epoch 71/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0546\n",
      "Epoch 72/100\n",
      "354/354 [==============================] - 0s 311us/step - loss: 0.0540\n",
      "Epoch 73/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0535\n",
      "Epoch 74/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0530\n",
      "Epoch 75/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0525\n",
      "Epoch 76/100\n",
      "354/354 [==============================] - 0s 311us/step - loss: 0.0519\n",
      "Epoch 77/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0515\n",
      "Epoch 78/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0510\n",
      "Epoch 79/100\n",
      "354/354 [==============================] - 0s 367us/step - loss: 0.0505\n",
      "Epoch 80/100\n",
      "354/354 [==============================] - 0s 339us/step - loss: 0.0500\n",
      "Epoch 81/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0495\n",
      "Epoch 82/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0490\n",
      "Epoch 83/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0484\n",
      "Epoch 84/100\n",
      "354/354 [==============================] - 0s 282us/step - loss: 0.0480\n",
      "Epoch 85/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0475\n",
      "Epoch 86/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0471\n",
      "Epoch 87/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0466\n",
      "Epoch 88/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0462\n",
      "Epoch 89/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0457\n",
      "Epoch 90/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0453\n",
      "Epoch 91/100\n",
      "354/354 [==============================] - 0s 339us/step - loss: 0.0449\n",
      "Epoch 92/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0447\n",
      "Epoch 93/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0441\n",
      "Epoch 94/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0436\n",
      "Epoch 95/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0433\n",
      "Epoch 96/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0429\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 0s 169us/step - loss: 0.0424\n",
      "Epoch 98/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0420\n",
      "Epoch 99/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0416\n",
      "Epoch 100/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x179e89e8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "m.fit(x_train_scaled, y_train, epochs=100, batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31., 18., 14., 21., 19., 13., 11., 19., 17., 23., 20., 13., 35.,\n",
       "        14., 13., 19., 19., 17., 13., 24., 13., 19.,  8., 15., 16., 18.,\n",
       "        33., 17., 27., 16., 21., 16., 26., 16., 16., 37., 11., 13., 20.,\n",
       "        13., 14., 17., 30., 25., 24., 16., 27., 28., 14., 15., 39., 12.,\n",
       "        15., 29., 19., 17., 30., 12., 23., 18., 16., 20., 16., 21., 31.,\n",
       "        20., 24., 31., 25., 14., 21., 24., 27., 20., 12., 22., 14.,  7.,\n",
       "        30., 19., 14., 16., 17., 14., 40., 14., 14., 36., 15., 20., 10.,\n",
       "        30., 27., 37., 16., 36., 14., 26., 20., 22., 19., 11., 16., 16.,\n",
       "        16., 15., 19., 22., 19., 26., 26., 35., 24., 25., 14., 16., 20.,\n",
       "        22., 17., 14., 11., 26., 21., 28., 17., 20., 25., 28., 24., 23.,\n",
       "        25., 17., 18., 18., 13., 28., 21., 23., 29., 32., 16., 32., 14.,\n",
       "        25., 25., 27., 18., 27., 23., 17., 12., 28.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_scaled = scaler.transform(x_test)\n",
    "m.predict(x_test_scaled).round().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([24.8, 18.7, 16.1, 21. , 19.3, 13.4, 15. , 24.5, 16.8, 15.3, 20.3,\n",
       "       13.4, 34.7, 18.7, 13.9, 17.1, 17.4, 21.8, 17.2, 20.7, 19.1, 22.2,\n",
       "        5. , 15.4, 21.4, 17.4, 26.6, 24.5, 22. , 20.8, 21.5, 16.1, 20.9,\n",
       "       19.5, 25. , 33.2,  7.2, 13.8, 20.1, 17.6, 10.9, 18.8, 28.7, 23.2,\n",
       "       22.2, 14.9, 23.7, 28. ,  9.7, 23. , 50. , 13.8, 13.8, 26.7, 18.2,\n",
       "       27.9, 32. ,  8.3, 18.9, 19.4, 11.5, 19.4, 20.4, 22. , 31.7, 21.2,\n",
       "       19.7, 33.1, 26.4, 13.9, 21.1, 23.1, 26.6, 20.3,  8.7, 23.1, 10.8,\n",
       "        7. , 25. , 17.3, 13.6, 17.7, 18.4, 13.1, 39.8, 20.1, 27.5, 50. ,\n",
       "        7.2, 15. , 13.2, 33.1, 28.7, 30.3, 14.4, 43.1, 10.2, 21.2, 20.6,\n",
       "       22.6, 21.7,  7.4, 13.1, 20. , 18.3, 21.2, 24.3, 19.8, 17.2, 22. ,\n",
       "       23.4, 29.1, 23.8, 16.8, 13.4, 23.7, 20.9, 20.4, 13.1, 20.2, 10.4,\n",
       "       22.4, 20.5, 24.5, 17.4, 18.5, 23.3, 23.5, 17. , 21.4, 21.7, 20.2,\n",
       "       15.6, 15.7,  8.3, 29.8, 19.3, 26.2, 22.6, 34.9,  7. , 30.1,  9.6,\n",
       "       31.2, 23.8, 21.9, 22.5, 23.9, 24.8, 15.6, 10.5, 22.8],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_test.as_matrix().astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 0s 66us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04481237793439313"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.392515</td>\n",
       "      <td>-1.414743</td>\n",
       "      <td>-0.101660</td>\n",
       "      <td>-0.126167</td>\n",
       "      <td>0.308211</td>\n",
       "      <td>0.662036</td>\n",
       "      <td>-1.078846</td>\n",
       "      <td>-0.541369</td>\n",
       "      <td>-0.238713</td>\n",
       "      <td>-0.266023</td>\n",
       "      <td>0.163918</td>\n",
       "      <td>-0.335499</td>\n",
       "      <td>0.851891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.453191</td>\n",
       "      <td>-0.100698</td>\n",
       "      <td>-0.089365</td>\n",
       "      <td>-0.632418</td>\n",
       "      <td>0.115651</td>\n",
       "      <td>1.078104</td>\n",
       "      <td>-1.531387</td>\n",
       "      <td>0.377335</td>\n",
       "      <td>0.863053</td>\n",
       "      <td>2.608763</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>-0.119799</td>\n",
       "      <td>-0.804623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.395931</td>\n",
       "      <td>-1.401558</td>\n",
       "      <td>-0.230626</td>\n",
       "      <td>-0.352590</td>\n",
       "      <td>-0.429168</td>\n",
       "      <td>-0.432216</td>\n",
       "      <td>-0.725908</td>\n",
       "      <td>-0.682336</td>\n",
       "      <td>0.064509</td>\n",
       "      <td>0.082179</td>\n",
       "      <td>-0.160269</td>\n",
       "      <td>-0.504282</td>\n",
       "      <td>1.044570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.210819</td>\n",
       "      <td>0.349217</td>\n",
       "      <td>-0.617643</td>\n",
       "      <td>0.803447</td>\n",
       "      <td>0.315714</td>\n",
       "      <td>-1.106328</td>\n",
       "      <td>-0.508760</td>\n",
       "      <td>-0.043647</td>\n",
       "      <td>-0.103709</td>\n",
       "      <td>-1.200269</td>\n",
       "      <td>-1.693939</td>\n",
       "      <td>-0.519410</td>\n",
       "      <td>0.318767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.080339</td>\n",
       "      <td>-0.307171</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>1.101466</td>\n",
       "      <td>-0.394107</td>\n",
       "      <td>0.621605</td>\n",
       "      <td>0.666964</td>\n",
       "      <td>0.697231</td>\n",
       "      <td>0.112262</td>\n",
       "      <td>0.957241</td>\n",
       "      <td>-0.503091</td>\n",
       "      <td>-0.183371</td>\n",
       "      <td>-1.534875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.398296</td>\n",
       "      <td>-1.414749</td>\n",
       "      <td>-0.266506</td>\n",
       "      <td>-0.404331</td>\n",
       "      <td>2.565216</td>\n",
       "      <td>0.825079</td>\n",
       "      <td>-0.586051</td>\n",
       "      <td>0.027675</td>\n",
       "      <td>-0.237983</td>\n",
       "      <td>-0.209530</td>\n",
       "      <td>-0.832116</td>\n",
       "      <td>-0.262309</td>\n",
       "      <td>-0.171976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.688746</td>\n",
       "      <td>-0.059293</td>\n",
       "      <td>1.122622</td>\n",
       "      <td>-0.977928</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-0.011667</td>\n",
       "      <td>-0.186677</td>\n",
       "      <td>-0.704124</td>\n",
       "      <td>-0.013854</td>\n",
       "      <td>1.409384</td>\n",
       "      <td>0.697131</td>\n",
       "      <td>-0.342991</td>\n",
       "      <td>-0.911074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.626751</td>\n",
       "      <td>-1.574050</td>\n",
       "      <td>0.305474</td>\n",
       "      <td>0.382014</td>\n",
       "      <td>-1.989601</td>\n",
       "      <td>0.951702</td>\n",
       "      <td>3.859384</td>\n",
       "      <td>2.905810</td>\n",
       "      <td>0.570543</td>\n",
       "      <td>-0.281707</td>\n",
       "      <td>-0.591855</td>\n",
       "      <td>0.106628</td>\n",
       "      <td>-0.675345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.192155</td>\n",
       "      <td>1.537002</td>\n",
       "      <td>-0.884819</td>\n",
       "      <td>-0.514621</td>\n",
       "      <td>-0.715615</td>\n",
       "      <td>-0.346599</td>\n",
       "      <td>2.327896</td>\n",
       "      <td>-0.137867</td>\n",
       "      <td>-1.399878</td>\n",
       "      <td>1.065959</td>\n",
       "      <td>1.021056</td>\n",
       "      <td>-0.648468</td>\n",
       "      <td>-0.910024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.044407</td>\n",
       "      <td>-0.439183</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.885994</td>\n",
       "      <td>-0.638872</td>\n",
       "      <td>-0.651136</td>\n",
       "      <td>-0.310476</td>\n",
       "      <td>4.405302</td>\n",
       "      <td>-1.077673</td>\n",
       "      <td>1.466195</td>\n",
       "      <td>-0.233679</td>\n",
       "      <td>0.286318</td>\n",
       "      <td>0.948082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.655730</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.223745</td>\n",
       "      <td>1.789685</td>\n",
       "      <td>0.039233</td>\n",
       "      <td>-0.652710</td>\n",
       "      <td>-0.258851</td>\n",
       "      <td>0.631954</td>\n",
       "      <td>-1.193224</td>\n",
       "      <td>-1.290091</td>\n",
       "      <td>0.467971</td>\n",
       "      <td>-0.170196</td>\n",
       "      <td>0.329710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.753756</td>\n",
       "      <td>-0.014045</td>\n",
       "      <td>0.769918</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>0.115987</td>\n",
       "      <td>-0.051187</td>\n",
       "      <td>-0.061845</td>\n",
       "      <td>0.048782</td>\n",
       "      <td>0.605268</td>\n",
       "      <td>-0.470455</td>\n",
       "      <td>0.413832</td>\n",
       "      <td>-0.067020</td>\n",
       "      <td>-0.209589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.737042</td>\n",
       "      <td>0.062178</td>\n",
       "      <td>0.416365</td>\n",
       "      <td>0.140543</td>\n",
       "      <td>0.047421</td>\n",
       "      <td>-0.192147</td>\n",
       "      <td>-0.197270</td>\n",
       "      <td>0.159237</td>\n",
       "      <td>0.774944</td>\n",
       "      <td>-0.171155</td>\n",
       "      <td>0.698078</td>\n",
       "      <td>-0.093957</td>\n",
       "      <td>-0.473910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.106012</td>\n",
       "      <td>-0.540869</td>\n",
       "      <td>-0.555101</td>\n",
       "      <td>-0.013254</td>\n",
       "      <td>-0.667664</td>\n",
       "      <td>-0.517171</td>\n",
       "      <td>0.224018</td>\n",
       "      <td>1.639668</td>\n",
       "      <td>-0.312549</td>\n",
       "      <td>-0.460405</td>\n",
       "      <td>0.862551</td>\n",
       "      <td>-0.120761</td>\n",
       "      <td>-0.473674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.730220</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>0.612535</td>\n",
       "      <td>-0.396838</td>\n",
       "      <td>-0.092265</td>\n",
       "      <td>-0.603903</td>\n",
       "      <td>-0.094739</td>\n",
       "      <td>1.561303</td>\n",
       "      <td>-0.612299</td>\n",
       "      <td>1.302154</td>\n",
       "      <td>-0.806235</td>\n",
       "      <td>-0.199696</td>\n",
       "      <td>0.187074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.566516</td>\n",
       "      <td>-0.096941</td>\n",
       "      <td>-1.422727</td>\n",
       "      <td>-0.931946</td>\n",
       "      <td>0.146392</td>\n",
       "      <td>0.868343</td>\n",
       "      <td>-1.103020</td>\n",
       "      <td>0.816706</td>\n",
       "      <td>1.606882</td>\n",
       "      <td>0.114288</td>\n",
       "      <td>-0.134377</td>\n",
       "      <td>-0.143944</td>\n",
       "      <td>0.062483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.059868</td>\n",
       "      <td>0.221689</td>\n",
       "      <td>-0.321367</td>\n",
       "      <td>1.169283</td>\n",
       "      <td>0.191701</td>\n",
       "      <td>-1.233269</td>\n",
       "      <td>-0.395587</td>\n",
       "      <td>0.485587</td>\n",
       "      <td>0.645470</td>\n",
       "      <td>0.944009</td>\n",
       "      <td>-0.483165</td>\n",
       "      <td>-0.259415</td>\n",
       "      <td>0.744438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.015035</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>-0.993389</td>\n",
       "      <td>-0.567745</td>\n",
       "      <td>0.018312</td>\n",
       "      <td>2.883718</td>\n",
       "      <td>0.457784</td>\n",
       "      <td>-0.109707</td>\n",
       "      <td>-2.292031</td>\n",
       "      <td>0.065681</td>\n",
       "      <td>0.453369</td>\n",
       "      <td>-0.535837</td>\n",
       "      <td>3.171662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.497633</td>\n",
       "      <td>-0.545180</td>\n",
       "      <td>0.102844</td>\n",
       "      <td>0.148823</td>\n",
       "      <td>4.791093</td>\n",
       "      <td>0.756234</td>\n",
       "      <td>0.410120</td>\n",
       "      <td>0.542513</td>\n",
       "      <td>-0.252777</td>\n",
       "      <td>-0.263808</td>\n",
       "      <td>-0.034314</td>\n",
       "      <td>0.058311</td>\n",
       "      <td>0.101026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.395866</td>\n",
       "      <td>-1.413971</td>\n",
       "      <td>-0.165060</td>\n",
       "      <td>-0.240669</td>\n",
       "      <td>2.300938</td>\n",
       "      <td>0.404390</td>\n",
       "      <td>-0.352747</td>\n",
       "      <td>-0.083122</td>\n",
       "      <td>-0.216118</td>\n",
       "      <td>-0.441447</td>\n",
       "      <td>-1.203400</td>\n",
       "      <td>-0.328296</td>\n",
       "      <td>-0.446273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.571908</td>\n",
       "      <td>-0.042107</td>\n",
       "      <td>0.385088</td>\n",
       "      <td>1.010272</td>\n",
       "      <td>0.201546</td>\n",
       "      <td>1.318580</td>\n",
       "      <td>-0.500397</td>\n",
       "      <td>0.106679</td>\n",
       "      <td>-1.748461</td>\n",
       "      <td>1.030316</td>\n",
       "      <td>0.280373</td>\n",
       "      <td>-0.283685</td>\n",
       "      <td>0.125296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.112419</td>\n",
       "      <td>-0.396050</td>\n",
       "      <td>1.416869</td>\n",
       "      <td>-0.317821</td>\n",
       "      <td>-0.415187</td>\n",
       "      <td>-0.336041</td>\n",
       "      <td>-0.004835</td>\n",
       "      <td>1.589929</td>\n",
       "      <td>-1.476736</td>\n",
       "      <td>-1.704726</td>\n",
       "      <td>0.329386</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-1.201285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.493330</td>\n",
       "      <td>-0.177059</td>\n",
       "      <td>1.336538</td>\n",
       "      <td>1.718892</td>\n",
       "      <td>0.061133</td>\n",
       "      <td>0.022833</td>\n",
       "      <td>-0.084685</td>\n",
       "      <td>-0.163322</td>\n",
       "      <td>0.496470</td>\n",
       "      <td>1.122049</td>\n",
       "      <td>-0.269827</td>\n",
       "      <td>-0.079147</td>\n",
       "      <td>0.442088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.963563</td>\n",
       "      <td>3.389473</td>\n",
       "      <td>0.257280</td>\n",
       "      <td>0.185555</td>\n",
       "      <td>-1.483705</td>\n",
       "      <td>-0.319478</td>\n",
       "      <td>-0.497105</td>\n",
       "      <td>-0.367309</td>\n",
       "      <td>0.302691</td>\n",
       "      <td>0.458770</td>\n",
       "      <td>0.335236</td>\n",
       "      <td>-0.157376</td>\n",
       "      <td>-0.173066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.219915</td>\n",
       "      <td>0.405413</td>\n",
       "      <td>-1.120181</td>\n",
       "      <td>0.137350</td>\n",
       "      <td>0.299450</td>\n",
       "      <td>1.795816</td>\n",
       "      <td>2.662048</td>\n",
       "      <td>-4.402920</td>\n",
       "      <td>0.779141</td>\n",
       "      <td>0.295552</td>\n",
       "      <td>0.031243</td>\n",
       "      <td>-1.143167</td>\n",
       "      <td>-0.511999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.386840</td>\n",
       "      <td>-1.365224</td>\n",
       "      <td>0.387433</td>\n",
       "      <td>0.668198</td>\n",
       "      <td>-0.660431</td>\n",
       "      <td>-0.294905</td>\n",
       "      <td>-0.534697</td>\n",
       "      <td>-0.902068</td>\n",
       "      <td>-0.083573</td>\n",
       "      <td>0.338354</td>\n",
       "      <td>-0.630267</td>\n",
       "      <td>-0.474948</td>\n",
       "      <td>-1.258235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.097586</td>\n",
       "      <td>0.277377</td>\n",
       "      <td>0.325347</td>\n",
       "      <td>0.641730</td>\n",
       "      <td>0.326856</td>\n",
       "      <td>-0.673378</td>\n",
       "      <td>-0.381150</td>\n",
       "      <td>-0.748872</td>\n",
       "      <td>-1.390231</td>\n",
       "      <td>0.546469</td>\n",
       "      <td>0.006480</td>\n",
       "      <td>-0.561589</td>\n",
       "      <td>-0.549601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.429948</td>\n",
       "      <td>0.062861</td>\n",
       "      <td>-0.280997</td>\n",
       "      <td>-0.978716</td>\n",
       "      <td>-0.205745</td>\n",
       "      <td>-0.510351</td>\n",
       "      <td>-0.779506</td>\n",
       "      <td>0.361896</td>\n",
       "      <td>1.309176</td>\n",
       "      <td>3.037761</td>\n",
       "      <td>0.190144</td>\n",
       "      <td>-0.268396</td>\n",
       "      <td>-0.585068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.516077</td>\n",
       "      <td>0.345113</td>\n",
       "      <td>-1.275936</td>\n",
       "      <td>-0.707710</td>\n",
       "      <td>0.195135</td>\n",
       "      <td>1.406034</td>\n",
       "      <td>-1.295874</td>\n",
       "      <td>0.857941</td>\n",
       "      <td>1.477943</td>\n",
       "      <td>0.054503</td>\n",
       "      <td>-0.321168</td>\n",
       "      <td>-0.072855</td>\n",
       "      <td>0.087608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.702860</td>\n",
       "      <td>0.494349</td>\n",
       "      <td>-0.436119</td>\n",
       "      <td>0.745541</td>\n",
       "      <td>-0.099263</td>\n",
       "      <td>-0.826899</td>\n",
       "      <td>-0.326913</td>\n",
       "      <td>1.532697</td>\n",
       "      <td>-0.026038</td>\n",
       "      <td>-1.031130</td>\n",
       "      <td>0.166979</td>\n",
       "      <td>-0.052276</td>\n",
       "      <td>-0.660693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>-0.757267</td>\n",
       "      <td>0.041086</td>\n",
       "      <td>-1.251846</td>\n",
       "      <td>-0.515543</td>\n",
       "      <td>-0.276741</td>\n",
       "      <td>-0.781799</td>\n",
       "      <td>1.196236</td>\n",
       "      <td>-0.017632</td>\n",
       "      <td>1.947221</td>\n",
       "      <td>-1.090843</td>\n",
       "      <td>-0.401865</td>\n",
       "      <td>-0.414917</td>\n",
       "      <td>0.886753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>-0.733363</td>\n",
       "      <td>-0.008299</td>\n",
       "      <td>-0.161637</td>\n",
       "      <td>-0.817979</td>\n",
       "      <td>-0.073810</td>\n",
       "      <td>-0.685685</td>\n",
       "      <td>-0.281228</td>\n",
       "      <td>0.303010</td>\n",
       "      <td>1.077122</td>\n",
       "      <td>0.321362</td>\n",
       "      <td>0.081721</td>\n",
       "      <td>-0.258444</td>\n",
       "      <td>-0.788982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>-0.940153</td>\n",
       "      <td>0.151186</td>\n",
       "      <td>-0.207779</td>\n",
       "      <td>-0.413499</td>\n",
       "      <td>0.177019</td>\n",
       "      <td>-0.134466</td>\n",
       "      <td>-0.415888</td>\n",
       "      <td>-0.310392</td>\n",
       "      <td>-0.006988</td>\n",
       "      <td>2.114669</td>\n",
       "      <td>0.704136</td>\n",
       "      <td>-0.413592</td>\n",
       "      <td>-0.652580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>-0.715233</td>\n",
       "      <td>-0.036414</td>\n",
       "      <td>0.284576</td>\n",
       "      <td>1.953082</td>\n",
       "      <td>0.144242</td>\n",
       "      <td>-0.191477</td>\n",
       "      <td>0.159873</td>\n",
       "      <td>-0.169887</td>\n",
       "      <td>0.624718</td>\n",
       "      <td>0.327902</td>\n",
       "      <td>0.071925</td>\n",
       "      <td>-0.156671</td>\n",
       "      <td>0.906634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>-0.129147</td>\n",
       "      <td>-0.419538</td>\n",
       "      <td>-0.280431</td>\n",
       "      <td>0.621449</td>\n",
       "      <td>-0.298920</td>\n",
       "      <td>0.273634</td>\n",
       "      <td>-0.098503</td>\n",
       "      <td>0.944484</td>\n",
       "      <td>0.098335</td>\n",
       "      <td>-1.455741</td>\n",
       "      <td>0.355188</td>\n",
       "      <td>-0.071200</td>\n",
       "      <td>0.652784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>-0.097226</td>\n",
       "      <td>-0.183276</td>\n",
       "      <td>1.907805</td>\n",
       "      <td>0.476473</td>\n",
       "      <td>-0.398156</td>\n",
       "      <td>-0.248313</td>\n",
       "      <td>0.196619</td>\n",
       "      <td>1.557774</td>\n",
       "      <td>-1.325053</td>\n",
       "      <td>0.263434</td>\n",
       "      <td>-0.286976</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>-0.149475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1.992750</td>\n",
       "      <td>3.583881</td>\n",
       "      <td>0.026337</td>\n",
       "      <td>-0.210591</td>\n",
       "      <td>-0.727037</td>\n",
       "      <td>-0.393200</td>\n",
       "      <td>-0.340685</td>\n",
       "      <td>-0.105042</td>\n",
       "      <td>0.347799</td>\n",
       "      <td>0.341174</td>\n",
       "      <td>-1.145020</td>\n",
       "      <td>-0.243998</td>\n",
       "      <td>0.730734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1.970345</td>\n",
       "      <td>3.435130</td>\n",
       "      <td>0.399480</td>\n",
       "      <td>0.389513</td>\n",
       "      <td>3.173949</td>\n",
       "      <td>-1.467121</td>\n",
       "      <td>1.708121</td>\n",
       "      <td>0.606135</td>\n",
       "      <td>0.438528</td>\n",
       "      <td>-0.020074</td>\n",
       "      <td>0.448173</td>\n",
       "      <td>0.127385</td>\n",
       "      <td>0.450640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1.948583</td>\n",
       "      <td>3.363297</td>\n",
       "      <td>0.865560</td>\n",
       "      <td>1.191605</td>\n",
       "      <td>-0.209127</td>\n",
       "      <td>-0.298678</td>\n",
       "      <td>0.245746</td>\n",
       "      <td>-0.310202</td>\n",
       "      <td>-0.014771</td>\n",
       "      <td>-0.683190</td>\n",
       "      <td>0.671930</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>-1.241679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>-0.617824</td>\n",
       "      <td>-0.018726</td>\n",
       "      <td>0.853400</td>\n",
       "      <td>2.836643</td>\n",
       "      <td>0.058936</td>\n",
       "      <td>0.340786</td>\n",
       "      <td>1.091935</td>\n",
       "      <td>-0.467333</td>\n",
       "      <td>0.221697</td>\n",
       "      <td>0.149922</td>\n",
       "      <td>0.315345</td>\n",
       "      <td>-0.111718</td>\n",
       "      <td>-0.605388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>-0.822816</td>\n",
       "      <td>0.096239</td>\n",
       "      <td>-0.159095</td>\n",
       "      <td>-0.340937</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>-0.183301</td>\n",
       "      <td>-0.705184</td>\n",
       "      <td>0.046976</td>\n",
       "      <td>-2.687955</td>\n",
       "      <td>-1.411897</td>\n",
       "      <td>2.675661</td>\n",
       "      <td>-0.374496</td>\n",
       "      <td>1.286272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1.425175</td>\n",
       "      <td>-1.143851</td>\n",
       "      <td>-0.187892</td>\n",
       "      <td>-0.321188</td>\n",
       "      <td>-0.971417</td>\n",
       "      <td>-2.662200</td>\n",
       "      <td>0.512267</td>\n",
       "      <td>-0.848218</td>\n",
       "      <td>0.555596</td>\n",
       "      <td>-0.173793</td>\n",
       "      <td>3.933898</td>\n",
       "      <td>-0.335265</td>\n",
       "      <td>-0.847802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-0.731995</td>\n",
       "      <td>0.029004</td>\n",
       "      <td>-1.330004</td>\n",
       "      <td>-0.652376</td>\n",
       "      <td>0.095673</td>\n",
       "      <td>0.752989</td>\n",
       "      <td>-0.455177</td>\n",
       "      <td>-0.230619</td>\n",
       "      <td>0.420814</td>\n",
       "      <td>0.629742</td>\n",
       "      <td>-0.025794</td>\n",
       "      <td>3.371736</td>\n",
       "      <td>-1.248983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.139721</td>\n",
       "      <td>-0.594806</td>\n",
       "      <td>-0.983557</td>\n",
       "      <td>-0.661859</td>\n",
       "      <td>-0.748495</td>\n",
       "      <td>0.884076</td>\n",
       "      <td>2.003715</td>\n",
       "      <td>-0.102339</td>\n",
       "      <td>1.572005</td>\n",
       "      <td>-0.677983</td>\n",
       "      <td>-1.119226</td>\n",
       "      <td>-0.458721</td>\n",
       "      <td>0.147004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1.422949</td>\n",
       "      <td>-1.170403</td>\n",
       "      <td>-0.165535</td>\n",
       "      <td>-0.258324</td>\n",
       "      <td>0.031948</td>\n",
       "      <td>-0.308061</td>\n",
       "      <td>-0.607836</td>\n",
       "      <td>-0.563206</td>\n",
       "      <td>0.038702</td>\n",
       "      <td>0.159958</td>\n",
       "      <td>-0.351387</td>\n",
       "      <td>3.384883</td>\n",
       "      <td>1.734662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.022646</td>\n",
       "      <td>0.106179</td>\n",
       "      <td>-0.999351</td>\n",
       "      <td>-0.596399</td>\n",
       "      <td>-0.144909</td>\n",
       "      <td>1.685397</td>\n",
       "      <td>1.127225</td>\n",
       "      <td>-0.160271</td>\n",
       "      <td>-2.016276</td>\n",
       "      <td>0.219298</td>\n",
       "      <td>0.547821</td>\n",
       "      <td>-0.618337</td>\n",
       "      <td>3.422920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>-0.728820</td>\n",
       "      <td>-0.029165</td>\n",
       "      <td>0.409205</td>\n",
       "      <td>0.128309</td>\n",
       "      <td>0.203628</td>\n",
       "      <td>-0.069007</td>\n",
       "      <td>-0.898835</td>\n",
       "      <td>-0.819606</td>\n",
       "      <td>1.153111</td>\n",
       "      <td>1.242039</td>\n",
       "      <td>0.802041</td>\n",
       "      <td>-0.215844</td>\n",
       "      <td>0.193065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1.920874</td>\n",
       "      <td>3.054046</td>\n",
       "      <td>0.429143</td>\n",
       "      <td>0.498309</td>\n",
       "      <td>-0.096519</td>\n",
       "      <td>0.592905</td>\n",
       "      <td>-0.533317</td>\n",
       "      <td>-0.175982</td>\n",
       "      <td>-0.154295</td>\n",
       "      <td>-0.706522</td>\n",
       "      <td>-1.907653</td>\n",
       "      <td>-0.181741</td>\n",
       "      <td>0.813162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>1.937960</td>\n",
       "      <td>3.117986</td>\n",
       "      <td>-0.017900</td>\n",
       "      <td>-0.245899</td>\n",
       "      <td>-0.328363</td>\n",
       "      <td>0.488284</td>\n",
       "      <td>-0.797724</td>\n",
       "      <td>-0.065658</td>\n",
       "      <td>0.070395</td>\n",
       "      <td>-0.170963</td>\n",
       "      <td>-1.245694</td>\n",
       "      <td>-0.199020</td>\n",
       "      <td>0.906832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>-0.809150</td>\n",
       "      <td>0.195638</td>\n",
       "      <td>-0.249363</td>\n",
       "      <td>-0.505748</td>\n",
       "      <td>0.095424</td>\n",
       "      <td>-0.808377</td>\n",
       "      <td>-0.445993</td>\n",
       "      <td>0.051776</td>\n",
       "      <td>-2.545373</td>\n",
       "      <td>-1.008523</td>\n",
       "      <td>-2.109563</td>\n",
       "      <td>-0.825022</td>\n",
       "      <td>0.338424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-0.907707</td>\n",
       "      <td>0.117010</td>\n",
       "      <td>1.989651</td>\n",
       "      <td>-1.664705</td>\n",
       "      <td>0.108697</td>\n",
       "      <td>-0.527126</td>\n",
       "      <td>-0.085685</td>\n",
       "      <td>-0.496085</td>\n",
       "      <td>-1.059720</td>\n",
       "      <td>-1.123837</td>\n",
       "      <td>-0.931373</td>\n",
       "      <td>-0.455697</td>\n",
       "      <td>-0.308445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.103840</td>\n",
       "      <td>-0.554436</td>\n",
       "      <td>-0.530651</td>\n",
       "      <td>0.027712</td>\n",
       "      <td>-0.671424</td>\n",
       "      <td>-0.529651</td>\n",
       "      <td>0.241137</td>\n",
       "      <td>1.624283</td>\n",
       "      <td>-0.353390</td>\n",
       "      <td>-0.573927</td>\n",
       "      <td>-0.424988</td>\n",
       "      <td>-0.225670</td>\n",
       "      <td>-0.406154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>-1.074735</td>\n",
       "      <td>0.275981</td>\n",
       "      <td>2.229952</td>\n",
       "      <td>-2.350010</td>\n",
       "      <td>0.204221</td>\n",
       "      <td>-0.252677</td>\n",
       "      <td>0.356981</td>\n",
       "      <td>-1.744477</td>\n",
       "      <td>-0.714600</td>\n",
       "      <td>-1.778657</td>\n",
       "      <td>-1.078280</td>\n",
       "      <td>-0.641510</td>\n",
       "      <td>-0.600478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>-0.733017</td>\n",
       "      <td>-0.019656</td>\n",
       "      <td>-1.540135</td>\n",
       "      <td>-0.986805</td>\n",
       "      <td>0.269530</td>\n",
       "      <td>1.591975</td>\n",
       "      <td>-1.028754</td>\n",
       "      <td>-0.109926</td>\n",
       "      <td>0.316166</td>\n",
       "      <td>0.705361</td>\n",
       "      <td>0.476856</td>\n",
       "      <td>3.455606</td>\n",
       "      <td>-1.621440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1.405434</td>\n",
       "      <td>-1.066213</td>\n",
       "      <td>1.205133</td>\n",
       "      <td>2.008575</td>\n",
       "      <td>-0.734792</td>\n",
       "      <td>0.169424</td>\n",
       "      <td>-0.351587</td>\n",
       "      <td>-1.108845</td>\n",
       "      <td>-0.342157</td>\n",
       "      <td>0.386203</td>\n",
       "      <td>0.186315</td>\n",
       "      <td>-0.268274</td>\n",
       "      <td>-0.508217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>-0.750338</td>\n",
       "      <td>0.140477</td>\n",
       "      <td>0.151479</td>\n",
       "      <td>1.789119</td>\n",
       "      <td>0.158965</td>\n",
       "      <td>0.528769</td>\n",
       "      <td>0.564258</td>\n",
       "      <td>-0.691189</td>\n",
       "      <td>-0.045710</td>\n",
       "      <td>-0.447035</td>\n",
       "      <td>-0.264151</td>\n",
       "      <td>-0.321670</td>\n",
       "      <td>-0.075203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.544721</td>\n",
       "      <td>0.131720</td>\n",
       "      <td>-1.165760</td>\n",
       "      <td>-0.529409</td>\n",
       "      <td>-0.037188</td>\n",
       "      <td>0.188266</td>\n",
       "      <td>-0.595267</td>\n",
       "      <td>0.729335</td>\n",
       "      <td>1.765362</td>\n",
       "      <td>0.524836</td>\n",
       "      <td>0.349613</td>\n",
       "      <td>-0.119902</td>\n",
       "      <td>0.526477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>-0.712968</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>-0.542020</td>\n",
       "      <td>0.605075</td>\n",
       "      <td>0.101148</td>\n",
       "      <td>-0.004499</td>\n",
       "      <td>-0.697558</td>\n",
       "      <td>0.214421</td>\n",
       "      <td>0.406367</td>\n",
       "      <td>-0.908783</td>\n",
       "      <td>0.954717</td>\n",
       "      <td>-0.135945</td>\n",
       "      <td>-0.176490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1.564817</td>\n",
       "      <td>0.010203</td>\n",
       "      <td>-0.027734</td>\n",
       "      <td>-0.075221</td>\n",
       "      <td>1.772899</td>\n",
       "      <td>1.686082</td>\n",
       "      <td>-1.089571</td>\n",
       "      <td>-0.014826</td>\n",
       "      <td>-0.398521</td>\n",
       "      <td>-0.615868</td>\n",
       "      <td>1.743979</td>\n",
       "      <td>0.073615</td>\n",
       "      <td>-0.263398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>-1.193875</td>\n",
       "      <td>0.416109</td>\n",
       "      <td>-1.010860</td>\n",
       "      <td>0.164469</td>\n",
       "      <td>0.478516</td>\n",
       "      <td>0.094963</td>\n",
       "      <td>-1.397160</td>\n",
       "      <td>0.164570</td>\n",
       "      <td>-0.172405</td>\n",
       "      <td>-0.911587</td>\n",
       "      <td>0.505619</td>\n",
       "      <td>-0.285607</td>\n",
       "      <td>-0.344007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0    1.392515 -1.414743 -0.101660 -0.126167  0.308211  0.662036 -1.078846   \n",
       "1   -0.453191 -0.100698 -0.089365 -0.632418  0.115651  1.078104 -1.531387   \n",
       "2    1.395931 -1.401558 -0.230626 -0.352590 -0.429168 -0.432216 -0.725908   \n",
       "3   -1.210819  0.349217 -0.617643  0.803447  0.315714 -1.106328 -0.508760   \n",
       "4   -0.080339 -0.307171  0.026021  1.101466 -0.394107  0.621605  0.666964   \n",
       "5    1.398296 -1.414749 -0.266506 -0.404331  2.565216  0.825079 -0.586051   \n",
       "6   -0.688746 -0.059293  1.122622 -0.977928  0.043478 -0.011667 -0.186677   \n",
       "7    1.626751 -1.574050  0.305474  0.382014 -1.989601  0.951702  3.859384   \n",
       "8    0.192155  1.537002 -0.884819 -0.514621 -0.715615 -0.346599  2.327896   \n",
       "9    0.044407 -0.439183  0.003075  0.885994 -0.638872 -0.651136 -0.310476   \n",
       "10  -0.655730  0.006581  0.223745  1.789685  0.039233 -0.652710 -0.258851   \n",
       "11  -0.753756 -0.014045  0.769918  0.731400  0.115987 -0.051187 -0.061845   \n",
       "12  -0.737042  0.062178  0.416365  0.140543  0.047421 -0.192147 -0.197270   \n",
       "13   0.106012 -0.540869 -0.555101 -0.013254 -0.667664 -0.517171  0.224018   \n",
       "14  -0.730220 -0.000655  0.612535 -0.396838 -0.092265 -0.603903 -0.094739   \n",
       "15  -0.566516 -0.096941 -1.422727 -0.931946  0.146392  0.868343 -1.103020   \n",
       "16  -1.059868  0.221689 -0.321367  1.169283  0.191701 -1.233269 -0.395587   \n",
       "17   0.015035  0.040900 -0.993389 -0.567745  0.018312  2.883718  0.457784   \n",
       "18   1.497633 -0.545180  0.102844  0.148823  4.791093  0.756234  0.410120   \n",
       "19   1.395866 -1.413971 -0.165060 -0.240669  2.300938  0.404390 -0.352747   \n",
       "20  -0.571908 -0.042107  0.385088  1.010272  0.201546  1.318580 -0.500397   \n",
       "21  -0.112419 -0.396050  1.416869 -0.317821 -0.415187 -0.336041 -0.004835   \n",
       "22  -0.493330 -0.177059  1.336538  1.718892  0.061133  0.022833 -0.084685   \n",
       "23   1.963563  3.389473  0.257280  0.185555 -1.483705 -0.319478 -0.497105   \n",
       "24  -1.219915  0.405413 -1.120181  0.137350  0.299450  1.795816  2.662048   \n",
       "25   1.386840 -1.365224  0.387433  0.668198 -0.660431 -0.294905 -0.534697   \n",
       "26  -1.097586  0.277377  0.325347  0.641730  0.326856 -0.673378 -0.381150   \n",
       "27  -0.429948  0.062861 -0.280997 -0.978716 -0.205745 -0.510351 -0.779506   \n",
       "28  -0.516077  0.345113 -1.275936 -0.707710  0.195135  1.406034 -1.295874   \n",
       "29  -0.702860  0.494349 -0.436119  0.745541 -0.099263 -0.826899 -0.326913   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "324 -0.757267  0.041086 -1.251846 -0.515543 -0.276741 -0.781799  1.196236   \n",
       "325 -0.733363 -0.008299 -0.161637 -0.817979 -0.073810 -0.685685 -0.281228   \n",
       "326 -0.940153  0.151186 -0.207779 -0.413499  0.177019 -0.134466 -0.415888   \n",
       "327 -0.715233 -0.036414  0.284576  1.953082  0.144242 -0.191477  0.159873   \n",
       "328 -0.129147 -0.419538 -0.280431  0.621449 -0.298920  0.273634 -0.098503   \n",
       "329 -0.097226 -0.183276  1.907805  0.476473 -0.398156 -0.248313  0.196619   \n",
       "330  1.992750  3.583881  0.026337 -0.210591 -0.727037 -0.393200 -0.340685   \n",
       "331  1.970345  3.435130  0.399480  0.389513  3.173949 -1.467121  1.708121   \n",
       "332  1.948583  3.363297  0.865560  1.191605 -0.209127 -0.298678  0.245746   \n",
       "333 -0.617824 -0.018726  0.853400  2.836643  0.058936  0.340786  1.091935   \n",
       "334 -0.822816  0.096239 -0.159095 -0.340937  0.252941 -0.183301 -0.705184   \n",
       "335  1.425175 -1.143851 -0.187892 -0.321188 -0.971417 -2.662200  0.512267   \n",
       "336 -0.731995  0.029004 -1.330004 -0.652376  0.095673  0.752989 -0.455177   \n",
       "337  0.139721 -0.594806 -0.983557 -0.661859 -0.748495  0.884076  2.003715   \n",
       "338  1.422949 -1.170403 -0.165535 -0.258324  0.031948 -0.308061 -0.607836   \n",
       "339  0.022646  0.106179 -0.999351 -0.596399 -0.144909  1.685397  1.127225   \n",
       "340 -0.728820 -0.029165  0.409205  0.128309  0.203628 -0.069007 -0.898835   \n",
       "341  1.920874  3.054046  0.429143  0.498309 -0.096519  0.592905 -0.533317   \n",
       "342  1.937960  3.117986 -0.017900 -0.245899 -0.328363  0.488284 -0.797724   \n",
       "343 -0.809150  0.195638 -0.249363 -0.505748  0.095424 -0.808377 -0.445993   \n",
       "344 -0.907707  0.117010  1.989651 -1.664705  0.108697 -0.527126 -0.085685   \n",
       "345  0.103840 -0.554436 -0.530651  0.027712 -0.671424 -0.529651  0.241137   \n",
       "346 -1.074735  0.275981  2.229952 -2.350010  0.204221 -0.252677  0.356981   \n",
       "347 -0.733017 -0.019656 -1.540135 -0.986805  0.269530  1.591975 -1.028754   \n",
       "348  1.405434 -1.066213  1.205133  2.008575 -0.734792  0.169424 -0.351587   \n",
       "349 -0.750338  0.140477  0.151479  1.789119  0.158965  0.528769  0.564258   \n",
       "350 -0.544721  0.131720 -1.165760 -0.529409 -0.037188  0.188266 -0.595267   \n",
       "351 -0.712968  0.231400 -0.542020  0.605075  0.101148 -0.004499 -0.697558   \n",
       "352  1.564817  0.010203 -0.027734 -0.075221  1.772899  1.686082 -1.089571   \n",
       "353 -1.193875  0.416109 -1.010860  0.164469  0.478516  0.094963 -1.397160   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0   -0.541369 -0.238713 -0.266023  0.163918 -0.335499  0.851891  \n",
       "1    0.377335  0.863053  2.608763  0.253700 -0.119799 -0.804623  \n",
       "2   -0.682336  0.064509  0.082179 -0.160269 -0.504282  1.044570  \n",
       "3   -0.043647 -0.103709 -1.200269 -1.693939 -0.519410  0.318767  \n",
       "4    0.697231  0.112262  0.957241 -0.503091 -0.183371 -1.534875  \n",
       "5    0.027675 -0.237983 -0.209530 -0.832116 -0.262309 -0.171976  \n",
       "6   -0.704124 -0.013854  1.409384  0.697131 -0.342991 -0.911074  \n",
       "7    2.905810  0.570543 -0.281707 -0.591855  0.106628 -0.675345  \n",
       "8   -0.137867 -1.399878  1.065959  1.021056 -0.648468 -0.910024  \n",
       "9    4.405302 -1.077673  1.466195 -0.233679  0.286318  0.948082  \n",
       "10   0.631954 -1.193224 -1.290091  0.467971 -0.170196  0.329710  \n",
       "11   0.048782  0.605268 -0.470455  0.413832 -0.067020 -0.209589  \n",
       "12   0.159237  0.774944 -0.171155  0.698078 -0.093957 -0.473910  \n",
       "13   1.639668 -0.312549 -0.460405  0.862551 -0.120761 -0.473674  \n",
       "14   1.561303 -0.612299  1.302154 -0.806235 -0.199696  0.187074  \n",
       "15   0.816706  1.606882  0.114288 -0.134377 -0.143944  0.062483  \n",
       "16   0.485587  0.645470  0.944009 -0.483165 -0.259415  0.744438  \n",
       "17  -0.109707 -2.292031  0.065681  0.453369 -0.535837  3.171662  \n",
       "18   0.542513 -0.252777 -0.263808 -0.034314  0.058311  0.101026  \n",
       "19  -0.083122 -0.216118 -0.441447 -1.203400 -0.328296 -0.446273  \n",
       "20   0.106679 -1.748461  1.030316  0.280373 -0.283685  0.125296  \n",
       "21   1.589929 -1.476736 -1.704726  0.329386 -0.013776 -1.201285  \n",
       "22  -0.163322  0.496470  1.122049 -0.269827 -0.079147  0.442088  \n",
       "23  -0.367309  0.302691  0.458770  0.335236 -0.157376 -0.173066  \n",
       "24  -4.402920  0.779141  0.295552  0.031243 -1.143167 -0.511999  \n",
       "25  -0.902068 -0.083573  0.338354 -0.630267 -0.474948 -1.258235  \n",
       "26  -0.748872 -1.390231  0.546469  0.006480 -0.561589 -0.549601  \n",
       "27   0.361896  1.309176  3.037761  0.190144 -0.268396 -0.585068  \n",
       "28   0.857941  1.477943  0.054503 -0.321168 -0.072855  0.087608  \n",
       "29   1.532697 -0.026038 -1.031130  0.166979 -0.052276 -0.660693  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "324 -0.017632  1.947221 -1.090843 -0.401865 -0.414917  0.886753  \n",
       "325  0.303010  1.077122  0.321362  0.081721 -0.258444 -0.788982  \n",
       "326 -0.310392 -0.006988  2.114669  0.704136 -0.413592 -0.652580  \n",
       "327 -0.169887  0.624718  0.327902  0.071925 -0.156671  0.906634  \n",
       "328  0.944484  0.098335 -1.455741  0.355188 -0.071200  0.652784  \n",
       "329  1.557774 -1.325053  0.263434 -0.286976  0.010378 -0.149475  \n",
       "330 -0.105042  0.347799  0.341174 -1.145020 -0.243998  0.730734  \n",
       "331  0.606135  0.438528 -0.020074  0.448173  0.127385  0.450640  \n",
       "332 -0.310202 -0.014771 -0.683190  0.671930  0.033621 -1.241679  \n",
       "333 -0.467333  0.221697  0.149922  0.315345 -0.111718 -0.605388  \n",
       "334  0.046976 -2.687955 -1.411897  2.675661 -0.374496  1.286272  \n",
       "335 -0.848218  0.555596 -0.173793  3.933898 -0.335265 -0.847802  \n",
       "336 -0.230619  0.420814  0.629742 -0.025794  3.371736 -1.248983  \n",
       "337 -0.102339  1.572005 -0.677983 -1.119226 -0.458721  0.147004  \n",
       "338 -0.563206  0.038702  0.159958 -0.351387  3.384883  1.734662  \n",
       "339 -0.160271 -2.016276  0.219298  0.547821 -0.618337  3.422920  \n",
       "340 -0.819606  1.153111  1.242039  0.802041 -0.215844  0.193065  \n",
       "341 -0.175982 -0.154295 -0.706522 -1.907653 -0.181741  0.813162  \n",
       "342 -0.065658  0.070395 -0.170963 -1.245694 -0.199020  0.906832  \n",
       "343  0.051776 -2.545373 -1.008523 -2.109563 -0.825022  0.338424  \n",
       "344 -0.496085 -1.059720 -1.123837 -0.931373 -0.455697 -0.308445  \n",
       "345  1.624283 -0.353390 -0.573927 -0.424988 -0.225670 -0.406154  \n",
       "346 -1.744477 -0.714600 -1.778657 -1.078280 -0.641510 -0.600478  \n",
       "347 -0.109926  0.316166  0.705361  0.476856  3.455606 -1.621440  \n",
       "348 -1.108845 -0.342157  0.386203  0.186315 -0.268274 -0.508217  \n",
       "349 -0.691189 -0.045710 -0.447035 -0.264151 -0.321670 -0.075203  \n",
       "350  0.729335  1.765362  0.524836  0.349613 -0.119902  0.526477  \n",
       "351  0.214421  0.406367 -0.908783  0.954717 -0.135945 -0.176490  \n",
       "352 -0.014826 -0.398521 -0.615868  1.743979  0.073615 -0.263398  \n",
       "353  0.164570 -0.172405 -0.911587  0.505619 -0.285607 -0.344007  \n",
       "\n",
       "[354 rows x 13 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = dec.PCA(n_components=13, whiten=True)\n",
    "pca.fit(x_train)\n",
    "x_train_white = pd.DataFrame(pca.transform(x_train), columns=x.columns)\n",
    "x_train_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "m.add(kr.layers.Dense(100, input_dim=13, activation=\"relu\"))\n",
    "m.add(kr.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "m.compile(loss=\"mean_squared_logarithmic_error\", optimizer=\"adam\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "354/354 [==============================] - 0s 876us/step - loss: 7.9428\n",
      "Epoch 2/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 5.2934\n",
      "Epoch 3/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 4.0106\n",
      "Epoch 4/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 3.2021\n",
      "Epoch 5/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 2.6168\n",
      "Epoch 6/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 2.1632\n",
      "Epoch 7/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 1.8064\n",
      "Epoch 8/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 1.5184\n",
      "Epoch 9/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 1.2847\n",
      "Epoch 10/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 1.0926\n",
      "Epoch 11/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.9339\n",
      "Epoch 12/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.8015\n",
      "Epoch 13/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.6900\n",
      "Epoch 14/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.5955\n",
      "Epoch 15/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.5163\n",
      "Epoch 16/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.4478\n",
      "Epoch 17/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.3902\n",
      "Epoch 18/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.3403\n",
      "Epoch 19/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.2982\n",
      "Epoch 20/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.2616\n",
      "Epoch 21/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.2303\n",
      "Epoch 22/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.2032\n",
      "Epoch 23/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1799\n",
      "Epoch 24/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.1599\n",
      "Epoch 25/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.1424\n",
      "Epoch 26/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1271\n",
      "Epoch 27/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.1140\n",
      "Epoch 28/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.1028\n",
      "Epoch 29/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0930\n",
      "Epoch 30/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0845\n",
      "Epoch 31/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0770\n",
      "Epoch 32/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0705\n",
      "Epoch 33/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0649\n",
      "Epoch 34/100\n",
      "354/354 [==============================] - 0s 311us/step - loss: 0.0601\n",
      "Epoch 35/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0559\n",
      "Epoch 36/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0522\n",
      "Epoch 37/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0491\n",
      "Epoch 38/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0463\n",
      "Epoch 39/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0440\n",
      "Epoch 40/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0418\n",
      "Epoch 41/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0400\n",
      "Epoch 42/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0384\n",
      "Epoch 43/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0370\n",
      "Epoch 44/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0357\n",
      "Epoch 45/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0346\n",
      "Epoch 46/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0336\n",
      "Epoch 47/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0328\n",
      "Epoch 48/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0320\n",
      "Epoch 49/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0312\n",
      "Epoch 50/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0306\n",
      "Epoch 51/100\n",
      "354/354 [==============================] - 0s 254us/step - loss: 0.0301\n",
      "Epoch 52/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0296\n",
      "Epoch 53/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0291\n",
      "Epoch 54/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0287\n",
      "Epoch 55/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0283\n",
      "Epoch 56/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0279\n",
      "Epoch 57/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0276\n",
      "Epoch 58/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0273\n",
      "Epoch 59/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0270\n",
      "Epoch 60/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0267\n",
      "Epoch 61/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0264\n",
      "Epoch 62/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0262\n",
      "Epoch 63/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0259\n",
      "Epoch 64/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0257\n",
      "Epoch 65/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0254\n",
      "Epoch 66/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0252\n",
      "Epoch 67/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0250\n",
      "Epoch 68/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0248\n",
      "Epoch 69/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0246\n",
      "Epoch 70/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0244\n",
      "Epoch 71/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0243\n",
      "Epoch 72/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0241\n",
      "Epoch 73/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0239\n",
      "Epoch 74/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0237\n",
      "Epoch 75/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0236\n",
      "Epoch 76/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0234\n",
      "Epoch 77/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0232\n",
      "Epoch 78/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0231\n",
      "Epoch 79/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0229\n",
      "Epoch 80/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0227\n",
      "Epoch 81/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0226\n",
      "Epoch 82/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0224\n",
      "Epoch 83/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0223\n",
      "Epoch 84/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0221\n",
      "Epoch 85/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0220\n",
      "Epoch 86/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0218\n",
      "Epoch 87/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0217\n",
      "Epoch 88/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0216\n",
      "Epoch 89/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0214\n",
      "Epoch 90/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0213\n",
      "Epoch 91/100\n",
      "354/354 [==============================] - 0s 226us/step - loss: 0.0211\n",
      "Epoch 92/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0210\n",
      "Epoch 93/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0209\n",
      "Epoch 94/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0208\n",
      "Epoch 95/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0207\n",
      "Epoch 96/100\n",
      "354/354 [==============================] - 0s 198us/step - loss: 0.0205\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 0s 198us/step - loss: 0.0204\n",
      "Epoch 98/100\n",
      "354/354 [==============================] - 0s 141us/step - loss: 0.0203\n",
      "Epoch 99/100\n",
      "354/354 [==============================] - 0s 169us/step - loss: 0.0202\n",
      "Epoch 100/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: 0.0201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17d4ba58>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "m.fit(x_train_white, y_train, epochs=100, batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21., 21., 34., 15., 20., 20., 24., 21., 20., 40., 33., 33., 20.,\n",
       "        23., 26., 21., 34., 24., 26., 34., 14., 19., 16., 11., 13., 29.,\n",
       "        25., 22., 18., 18., 22., 26., 29., 22., 12., 18., 25., 23., 20.,\n",
       "        26., 11., 23., 24., 25., 23., 13., 26., 21., 19., 15., 13., 27.,\n",
       "        30., 26., 20., 17., 20., 29., 25., 19., 17., 31., 26., 28., 15.,\n",
       "        21., 27., 28., 30., 11., 27., 22., 21., 29., 27., 29., 16., 25.,\n",
       "        25., 33., 23., 12., 25., 23., 16., 16., 17., 28., 11., 21., 25.,\n",
       "        20., 23., 24., 31., 18., 12., 28., 26., 29., 20., 18., 11., 12.,\n",
       "        22., 15., 23., 26., 26., 21., 29., 26., 21., 23., 28., 13., 27.,\n",
       "        24., 40., 30., 23., 19., 33., 22., 24., 24., 22., 25., 33., 33.,\n",
       "        33., 11., 33., 35., 15., 23., 28., 21., 23., 17., 33., 22., 24.,\n",
       "        18., 19., 28., 27., 32., 24., 28., 19., 20.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_test_white = scaler.transform(x_test)\n",
    "m.predict(x_test_white).round().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([24.8, 18.7, 16.1, 21. , 19.3, 13.4, 15. , 24.5, 16.8, 15.3, 20.3,\n",
       "       13.4, 34.7, 18.7, 13.9, 17.1, 17.4, 21.8, 17.2, 20.7, 19.1, 22.2,\n",
       "        5. , 15.4, 21.4, 17.4, 26.6, 24.5, 22. , 20.8, 21.5, 16.1, 20.9,\n",
       "       19.5, 25. , 33.2,  7.2, 13.8, 20.1, 17.6, 10.9, 18.8, 28.7, 23.2,\n",
       "       22.2, 14.9, 23.7, 28. ,  9.7, 23. , 50. , 13.8, 13.8, 26.7, 18.2,\n",
       "       27.9, 32. ,  8.3, 18.9, 19.4, 11.5, 19.4, 20.4, 22. , 31.7, 21.2,\n",
       "       19.7, 33.1, 26.4, 13.9, 21.1, 23.1, 26.6, 20.3,  8.7, 23.1, 10.8,\n",
       "        7. , 25. , 17.3, 13.6, 17.7, 18.4, 13.1, 39.8, 20.1, 27.5, 50. ,\n",
       "        7.2, 15. , 13.2, 33.1, 28.7, 30.3, 14.4, 43.1, 10.2, 21.2, 20.6,\n",
       "       22.6, 21.7,  7.4, 13.1, 20. , 18.3, 21.2, 24.3, 19.8, 17.2, 22. ,\n",
       "       23.4, 29.1, 23.8, 16.8, 13.4, 23.7, 20.9, 20.4, 13.1, 20.2, 10.4,\n",
       "       22.4, 20.5, 24.5, 17.4, 18.5, 23.3, 23.5, 17. , 21.4, 21.7, 20.2,\n",
       "       15.6, 15.7,  8.3, 29.8, 19.3, 26.2, 22.6, 34.9,  7. , 30.1,  9.6,\n",
       "       31.2, 23.8, 21.9, 22.5, 23.9, 24.8, 15.6, 10.5, 22.8],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_test.as_matrix().astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23299342550729452"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "m.evaluate(x_test_white, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
