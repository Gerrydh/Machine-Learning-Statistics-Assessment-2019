{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.models' from 'C:\\\\Users\\\\gerardh\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\keras\\\\models.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0xdefb240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']= (10,10) # make the plots a bit bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#create a new neural network\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "#Add a single neuron in a single layer, initialised with weight 1 and bias 0\n",
    "m.add(kr.layers.Dense(1, input_dim=1, activation='linear', kernel_initializer='ones', bias_initializer='zeros'))\n",
    "\n",
    "#complie the model\n",
    "m.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create some input values\n",
    "x = np.arange(0.0, 10.0, 1)\n",
    "\n",
    "#Run each x value through the neural network\n",
    "y = m.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAJCCAYAAADdrPONAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,y, 'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input are on the x axis and the outputs on the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a new neural network\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "#Add a single neuron in a single layer, initialised with weight 2 and bias 1.\n",
    "m.add(kr.layers.Dense(1, input_dim=1, activation=\"linear\", kernel_initializer=kr.initializers.Constant(value=2), bias_initializer=kr.initializers.Constant(value=1)))\n",
    "\n",
    "#Compile the model.\n",
    "m.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create some input values.\n",
    "x = np.arange(0.0, 10.0, 1)\n",
    "\n",
    "#Run each x value through the neural network\n",
    "y = m.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAJCCAYAAAD6AnJlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDUQXAEAD0QUA0OD/AbtfNGSLjQ+4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,y, 'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = 2x + 1\n",
    "\n",
    "example\n",
    "\n",
    "5 = 2 times 5 + 1 = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a new neural network\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "#Add a two neurons in a single layer\n",
    "m.add(kr.layers.Dense(2, input_dim=1, activation=\"linear\"))\n",
    "\n",
    "#Add a single neuron in a single layer, initialised with weight 1 and bias 0.\n",
    "m.add(kr.layers.Dense(1, activation=\"linear\", kernel_initializer=kr.initializers.Constant(value=1), bias_initializer=kr.initializers.Constant(value=0)))\n",
    "\n",
    "#Set the weight/bias of the two neurons\n",
    "m.layers[0].set_weights([np.matrix([2, 3]), np.array([-5, -3])])\n",
    "\n",
    "#compile the model\n",
    "m.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create some input values\n",
    "x = np.arange(0.0, 10.0, 1)\n",
    "\n",
    "#Run each x value through the neural network\n",
    "y = m.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAJCCAYAAAD3HAIiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new neural network\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "#Add a single neuron in a single layer, initialised with weight 1 and bias 0, with sigmod activation\n",
    "m.add(kr.layers.Dense(1, input_dim=1, activation=\"sigmoid\", kernel_initializer=kr.initializers.Constant(value=1), bias_initializer=kr.initializers.Constant(value=0)))\n",
    "\n",
    "# Compile the model\n",
    "m.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some input values\n",
    "x = np.arange(-10.0, 10.0, 1)\n",
    "\n",
    "# Run each x value through the neural network\n",
    "y = m.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJCCAYAAAAC4omSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJsIKAKDJ/wdkx+Ija91efwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['MEDV'] = boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "      <td>21.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "      <td>21.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "      <td>20.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0     0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14   0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14   0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14   0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14   0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14   0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14   0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14   0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14   0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14   0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10   0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10   0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10   0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10   0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10   0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10   0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10   0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10   0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10   0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10   0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10   0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10   0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74   0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74   0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74   0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74   0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74   0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69   0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69   0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69   0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69   0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69   0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69   0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69   0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69   0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93   0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93   0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93   0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93   0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93   0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "5       18.7  394.12   5.21  28.7  \n",
       "6       15.2  395.60  12.43  22.9  \n",
       "7       15.2  396.90  19.15  27.1  \n",
       "8       15.2  386.63  29.93  16.5  \n",
       "9       15.2  386.71  17.10  18.9  \n",
       "10      15.2  392.52  20.45  15.0  \n",
       "11      15.2  396.90  13.27  18.9  \n",
       "12      15.2  390.50  15.71  21.7  \n",
       "13      21.0  396.90   8.26  20.4  \n",
       "14      21.0  380.02  10.26  18.2  \n",
       "15      21.0  395.62   8.47  19.9  \n",
       "16      21.0  386.85   6.58  23.1  \n",
       "17      21.0  386.75  14.67  17.5  \n",
       "18      21.0  288.99  11.69  20.2  \n",
       "19      21.0  390.95  11.28  18.2  \n",
       "20      21.0  376.57  21.02  13.6  \n",
       "21      21.0  392.53  13.83  19.6  \n",
       "22      21.0  396.90  18.72  15.2  \n",
       "23      21.0  394.54  19.88  14.5  \n",
       "24      21.0  394.33  16.30  15.6  \n",
       "25      21.0  303.42  16.51  13.9  \n",
       "26      21.0  376.88  14.81  16.6  \n",
       "27      21.0  306.38  17.28  14.8  \n",
       "28      21.0  387.94  12.80  18.4  \n",
       "29      21.0  380.23  11.98  21.0  \n",
       "..       ...     ...    ...   ...  \n",
       "476     20.2  396.21  18.68  16.7  \n",
       "477     20.2  349.48  24.91  12.0  \n",
       "478     20.2  379.70  18.03  14.6  \n",
       "479     20.2  383.32  13.11  21.4  \n",
       "480     20.2  396.90  10.74  23.0  \n",
       "481     20.2  393.07   7.74  23.7  \n",
       "482     20.2  395.28   7.01  25.0  \n",
       "483     20.2  392.92  10.42  21.8  \n",
       "484     20.2  370.73  13.34  20.6  \n",
       "485     20.2  388.62  10.58  21.2  \n",
       "486     20.2  392.68  14.98  19.1  \n",
       "487     20.2  388.22  11.45  20.6  \n",
       "488     20.1  395.09  18.06  15.2  \n",
       "489     20.1  344.05  23.97   7.0  \n",
       "490     20.1  318.43  29.68   8.1  \n",
       "491     20.1  390.11  18.07  13.6  \n",
       "492     20.1  396.90  13.35  20.1  \n",
       "493     19.2  396.90  12.01  21.8  \n",
       "494     19.2  396.90  13.59  24.5  \n",
       "495     19.2  393.29  17.60  23.1  \n",
       "496     19.2  396.90  21.14  19.7  \n",
       "497     19.2  396.90  14.10  18.3  \n",
       "498     19.2  396.90  12.92  21.2  \n",
       "499     19.2  395.77  15.10  17.5  \n",
       "500     19.2  396.90  14.33  16.8  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 3.9690e+02, 4.9800e+00,\n",
       "        2.4000e+01],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 3.9690e+02, 9.1400e+00,\n",
       "        2.1600e+01],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 3.9283e+02, 4.0300e+00,\n",
       "        3.4700e+01],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 3.9690e+02, 5.6400e+00,\n",
       "        2.3900e+01],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 3.9345e+02, 6.4800e+00,\n",
       "        2.2000e+01],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 3.9690e+02, 7.8800e+00,\n",
       "        1.1900e+01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset[:,0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.80000000e-01, 6.78152493e-02, ...,\n",
       "        2.87234043e-01, 1.00000000e+00, 8.96799117e-02],\n",
       "       [2.35922539e-04, 0.00000000e+00, 2.42302053e-01, ...,\n",
       "        5.53191489e-01, 1.00000000e+00, 2.04470199e-01],\n",
       "       [2.35697744e-04, 0.00000000e+00, 2.42302053e-01, ...,\n",
       "        5.53191489e-01, 9.89737254e-01, 6.34657837e-02],\n",
       "       ...,\n",
       "       [6.11892474e-04, 0.00000000e+00, 4.20454545e-01, ...,\n",
       "        8.93617021e-01, 1.00000000e+00, 1.07891832e-01],\n",
       "       [1.16072990e-03, 0.00000000e+00, 4.20454545e-01, ...,\n",
       "        8.93617021e-01, 9.91300620e-01, 1.31070640e-01],\n",
       "       [4.61841693e-04, 0.00000000e+00, 4.20454545e-01, ...,\n",
       "        8.93617021e-01, 1.00000000e+00, 1.69701987e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13) (76, 13) (76, 13) (354,) (76,) (76,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(13,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 354 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "354/354 [==============================] - 0s 742us/step - loss: -249.3129 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "354/354 [==============================] - 0s 78us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "354/354 [==============================] - 0s 92us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "354/354 [==============================] - 0s 78us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "354/354 [==============================] - 0s 92us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "354/354 [==============================] - 0s 92us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "354/354 [==============================] - 0s 92us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "354/354 [==============================] - 0s 64us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "354/354 [==============================] - 0s 92us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "354/354 [==============================] - 0s 78us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "354/354 [==============================] - 0s 78us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "354/354 [==============================] - 0s 64us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "354/354 [==============================] - 0s 56us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "354/354 [==============================] - 0s 73us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "354/354 [==============================] - 0s 102us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "354/354 [==============================] - 0s 79us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "354/354 [==============================] - 0s 73us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "354/354 [==============================] - 0s 113us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "354/354 [==============================] - 0s 96us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "354/354 [==============================] - 0s 99us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "354/354 [==============================] - 0s 88us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "354/354 [==============================] - 0s 138us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "354/354 [==============================] - 0s 65us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "354/354 [==============================] - 0s 90us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "354/354 [==============================] - 0s 73us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "354/354 [==============================] - 0s 68us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "354/354 [==============================] - 0s 93us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "354/354 [==============================] - 0s 73us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "354/354 [==============================] - ETA: 0s - loss: -377.3363 - accuracy: 0.0000e+0 - 0s 96us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "354/354 [==============================] - 0s 65us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 0s 62us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "354/354 [==============================] - 0s 82us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "354/354 [==============================] - 0s 82us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "354/354 [==============================] - 0s 110us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "354/354 [==============================] - 0s 54us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "354/354 [==============================] - 0s 79us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "354/354 [==============================] - 0s 82us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "354/354 [==============================] - 0s 90us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "354/354 [==============================] - 0s 68us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "354/354 [==============================] - 0s 73us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "354/354 [==============================] - 0s 93us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "354/354 [==============================] - 0s 68us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "354/354 [==============================] - 0s 82us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "354/354 [==============================] - 0s 73us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "354/354 [==============================] - 0s 96us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "354/354 [==============================] - 0s 73us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "354/354 [==============================] - 0s 82us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "354/354 [==============================] - 0s 68us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "354/354 [==============================] - 0s 79us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "354/354 [==============================] - 0s 82us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "354/354 [==============================] - 0s 85us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "354/354 [==============================] - 0s 62us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "354/354 [==============================] - 0s 82us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "354/354 [==============================] - 0s 73us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "354/354 [==============================] - 0s 65us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "354/354 [==============================] - 0s 62us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "354/354 [==============================] - 0s 93us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "354/354 [==============================] - 0s 68us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "354/354 [==============================] - 0s 68us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "354/354 [==============================] - 0s 90us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "354/354 [==============================] - 0s 551us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "354/354 [==============================] - 0s 79us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "354/354 [==============================] - 0s 79us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "354/354 [==============================] - 0s 65us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "354/354 [==============================] - 0s 65us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "354/354 [==============================] - 0s 119us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerardh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.154015). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "354/354 [==============================] - 0s 82us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "354/354 [==============================] - 0s 71us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "354/354 [==============================] - 0s 62us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "354/354 [==============================] - 0s 79us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "354/354 [==============================] - 0s 76us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "354/354 [==============================] - 0s 62us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "354/354 [==============================] - 0s 79us/step - loss: -345.5084 - accuracy: 0.0000e+00 - val_loss: -314.9041 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 132us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzOrm0PEzMzq9v8BeAKkTGVok1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
